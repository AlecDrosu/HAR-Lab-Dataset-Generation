{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, LSTM, TimeDistributed, Reshape, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from oldslidingWindow import read_data, segment_data_by_day, sliding_window\n",
    "FILE_PATH = '../Processed Data/Aruba_17/processed_data.csv'\n",
    "\n",
    "data_df = read_data(FILE_PATH)\n",
    "daily_segments = segment_data_by_day(data_df)\n",
    "# only use the first 10 days\n",
    "# daily_segments = daily_segments[:120]\n",
    "window_size = 7816\n",
    "overlap_ratio = 0.2\n",
    "windows = sliding_window(daily_segments, window_size=window_size, overlap_ratio=overlap_ratio)\n",
    "\n",
    "# Prepare the data\n",
    "windows = np.asarray([window.to_numpy() for window in windows])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "timesteps = 7816\n",
    "input_dim = windows[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new train-test split using the windows\n",
    "window_train, window_val = train_test_split(windows, test_size=validation_split, shuffle=False)\n",
    "\n",
    "# Prepare the input data for the model by concatenating the windows along the time axis\n",
    "X_train = np.concatenate(window_train, axis=0)\n",
    "X_val = np.concatenate(window_val, axis=0)\n",
    "\n",
    "# Normalize the data using minMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train = X_train.reshape((-1, timesteps, input_dim))\n",
    "X_val = X_val.reshape((-1, timesteps, input_dim))\n",
    "\n",
    "# Generator model\n",
    "generator = Sequential()\n",
    "generator.add(Dense(256, activation='relu', input_shape=(timesteps, input_dim)))\n",
    "generator.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "generator.add(TimeDistributed(Dense(input_dim)))\n",
    "\n",
    "# Discriminator model\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(timesteps, input_dim)))\n",
    "discriminator.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# GAN model\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(timesteps, input_dim))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the GAN\n",
    "epochs = 300\n",
    "num_batches = X_train.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # print(f'Epoch: {epoch + 1}/{epochs}')\n",
    "    for batch in range(num_batches):\n",
    "        # Train the discriminator\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, timesteps, input_dim))\n",
    "        generated_sequences = generator.predict(noise)\n",
    "        real_sequences = X_train[batch * batch_size:(batch + 1) * batch_size]\n",
    "\n",
    "        x_combined = np.concatenate((real_sequences, generated_sequences))\n",
    "        y_combined = np.concatenate((np.ones((batch_size, timesteps, 1)), np.zeros((batch_size, timesteps, 1))))\n",
    "\n",
    "        d_loss = discriminator.train_on_batch(x_combined, y_combined)\n",
    "\n",
    "        # Train the generator\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, timesteps, input_dim))\n",
    "        y_mislabeled = np.ones((batch_size, timesteps, 1))\n",
    "\n",
    "        g_loss = gan.train_on_batch(noise, y_mislabeled)\n",
    "\n",
    "        # print(f'Batch {batch + 1}/{num_batches} - D loss: {d_loss:.4f} - G loss: {g_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the generator to create synthetic data\n",
    "noise = np.random.normal(0, 1, size=(X_val.shape[0], timesteps, input_dim))\n",
    "generated_data = generator.predict(noise)\n",
    "\n",
    "# Inverse transform the generated data\n",
    "generated_data_inverse = scaler.inverse_transform(generated_data.reshape(-1, input_dim))\n",
    "generated_data_inverse = np.rint(generated_data_inverse)\n",
    "\n",
    "# Save the generated data to a text file\n",
    "np.savetxt('generated_data.txt', generated_data_inverse, fmt='%.8f', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
