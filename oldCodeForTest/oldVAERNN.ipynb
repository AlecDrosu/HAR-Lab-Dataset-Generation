{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, TimeDistributed, Reshape, Bidirectional\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from oldslidingWindow import read_data, segment_data_by_day, sliding_window\n",
    "from PROCESSING import pre_processed_data, model_processing_code, model_post_processing, print_mappings, reconstructed_data, undo_split\n",
    "from oldCustom_layers import CustomPenaltyLayer\n",
    "from datetime import datetime\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eastern = pytz.timezone('US/Eastern')\n",
    "current_time = datetime.now(eastern).strftime('%m%d')\n",
    "\n",
    "RAW_DATA = '../Raw Data/Aruba_17/data'\n",
    "PRE_PROCESSED = '../Processed Data/Aruba_17/pre_processed_data.csv'\n",
    "FILE_PATH = '../Processed Data/Aruba_17/processed_data.csv'\n",
    "PREDICTIONS = f'../Predictions/Aruba_17_prediction_{current_time}.txt'\n",
    "COMPLETE_PREDICTION = f'../Predictions/Aruba_17_completed_prediction_{current_time}.txt'\n",
    "VAL_INPUT = f'../Predictions/Aruba_{current_time}.txt'\n",
    "pre_processed_data(RAW_DATA, PRE_PROCESSED)\n",
    "mappings = model_processing_code(PRE_PROCESSED, FILE_PATH)\n",
    "print_mappings(mappings['device_id_and_status_encoder'], mappings['activity_and_status_encoder'])\n",
    "print(mappings['activity_and_status_encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_df = read_data(FILE_PATH)\n",
    "daily_segments = segment_data_by_day(data_df)\n",
    "# Use the first 10 days\n",
    "daily_segments = daily_segments[:50]\n",
    "# Find the average length of the segments\n",
    "activities = []\n",
    "# write a loop that determines the number of activities per day, and appends to the activities list. Ignore the activity value of 22 since this correspoinds to a NaN value\n",
    "for segment in daily_segments:\n",
    "    for activity in segment[\"Activity\"]:\n",
    "        if activity != 22:\n",
    "            activities.append(activity)\n",
    "print(len(activities))\n",
    "print(len(set(activities)))\n",
    "print(activities)\n",
    "print(len(daily_segments))\n",
    "# print the average amount of activities per day\n",
    "print(len(activities)/len(daily_segments))\n",
    "avg_length = 0\n",
    "for segment in daily_segments:\n",
    "    avg_length += len(segment)\n",
    "avg_length /= len(daily_segments)\n",
    "avg_length = int(avg_length)\n",
    "print(\"The average length per day is\",avg_length)\n",
    "window_size = avg_length\n",
    "overlap_ratio = 0.2\n",
    "windows = sliding_window(daily_segments, window_size=window_size, overlap_ratio=overlap_ratio)\n",
    "print(len(windows))\n",
    "# Print the length of a single window\n",
    "print(\"The length of each window is\",len(windows[1]))\n",
    "# Prepare the data\n",
    "window_labels = []\n",
    "windows = np.asarray([window.to_numpy() for window in windows])\n",
    "date_column = windows[:, :, :1]\n",
    "windows = windows[:, :, 1:]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "batch_size = 16  \n",
    "validation_split = 0.3\n",
    "timesteps = window_size\n",
    "input_dim = windows[0].shape[1]\n",
    "print(windows.shape)\n",
    "print(input_dim)\n",
    "\n",
    "# print(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new train-test split using the windows\n",
    "window_train, window_val = train_test_split(windows, test_size=validation_split, shuffle=False)\n",
    "\n",
    "print(window_train.shape)\n",
    "print(window_val.shape)\n",
    "\n",
    "# Prepare the input data for the model by concatenating the windows along the time axis\n",
    "X_train = np.concatenate(window_train, axis=0)\n",
    "X_val = np.concatenate(window_val, axis=0)\n",
    "\n",
    "# Normalize the data using minMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train = X_train.reshape((-1, timesteps, input_dim))\n",
    "X_val = X_val.reshape((-1, timesteps, input_dim))\n",
    "\n",
    "latent_dim = 2\n",
    "encoding_dim = 32\n",
    "\n",
    "# ==================== ENCODER ====================\n",
    "inputs = Input(shape=(timesteps, input_dim), name='encoder_input')\n",
    "x = Bidirectional(LSTM(encoding_dim*2, return_sequences=True))(inputs)\n",
    "x = Bidirectional(LSTM(encoding_dim*2, return_sequences=False))(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "# z_mean is the mean of the latent space\n",
    "# z_log_var is the variance of the latent space\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "# ================= LATENT SPACE ==================\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "# ==================== DECODER ====================\n",
    "x = Dense(timesteps * encoding_dim, activation='relu')(latent_inputs)\n",
    "x = Reshape((timesteps, encoding_dim))(x)\n",
    "x = Bidirectional(LSTM(encoding_dim*2, return_sequences=True, input_shape=(timesteps, encoding_dim)))(x)\n",
    "x = TimeDistributed(Dense(input_dim))(x)\n",
    "# LSTM layer in the decoder is used to reconstruct the original sequence\n",
    "# print the summary\n",
    "\n",
    "# the VAE model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = Model(latent_inputs, x, name='decoder')\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "# print the summaries\n",
    "# encoder.summary()\n",
    "# decoder.summary()\n",
    "# vae.summary()\n",
    "\n",
    "# VAE loss function with custom_penalty\n",
    "reconstruction_loss = K.mean(K.square(inputs - outputs))\n",
    "reconstruction_loss *= timesteps * input_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "\n",
    "# Add the custom penalty to the loss function\n",
    "penalty_weight = 10.0  # Adjust the weight of the penalty term as needed\n",
    "penalty_layer = CustomPenaltyLayer(scaler, input_dim, timesteps)\n",
    "penalty = penalty_layer(outputs)\n",
    "penalty *= penalty_weight\n",
    "\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss + penalty)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "# vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "history = vae.fit(X_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, None))\n",
    "\n",
    "# Use the encoder to generate embeddings for each sequence\n",
    "encoder_model = Model(inputs, z_mean)\n",
    "X_embedded = encoder_model.predict(X_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fake dataset using the VAE model (SLIDING WINDOWS)\n",
    "\n",
    "n_samples = len(windows)\n",
    "\n",
    "# Sample from the latent space\n",
    "z_samples = np.random.normal(size=(n_samples, latent_dim))\n",
    "\n",
    "# Use the decoder to generate the output\n",
    "predicted_values = decoder.predict(z_samples)\n",
    "predicted_values = np.reshape(predicted_values, (n_samples, window_size, input_dim))\n",
    "\n",
    "# Undo the normalization\n",
    "predicted_values = np.reshape(predicted_values, (-1, input_dim))\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Reshape the predicted_values back into the window format\n",
    "predicted_windows = np.reshape(predicted_values, (n_samples, window_size, input_dim))\n",
    "predicted_windows_with_date_time = np.concatenate((date_column, predicted_windows), axis=2)\n",
    "\n",
    "print(len(predicted_windows_with_date_time))\n",
    "# print the types of the predicted_windows_with_date_time, window_size and overlap_ratio\n",
    "print(type(predicted_windows_with_date_time))\n",
    "print(type(window_size))\n",
    "print(type(overlap_ratio))\n",
    "\n",
    "original_data_format = reconstructed_data(predicted_windows_with_date_time, window_size, overlap_ratio)\n",
    "\n",
    "print(original_data_format)\n",
    "np.savetxt(PREDICTIONS, original_data_format, fmt='%s', delimiter=',', header='Date,Time,Device_Status,Activity', comments='')\n",
    "\n",
    "# print(reconstructed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to the original format\n",
    "model_post_processing(PREDICTIONS, COMPLETE_PREDICTION, mappings)\n",
    "undo_split(COMPLETE_PREDICTION, VAL_INPUT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
