{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "7816\n",
      "187\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, TimeDistributed, Reshape, Bidirectional\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from oldslidingWindow import read_data, segment_data_by_day, sliding_window\n",
    "from oldCustom_layers import CustomPenaltyLayer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "FILE_PATH = '../Processed Data/Aruba_17/processed_data.csv'\n",
    "\n",
    "data_df = read_data(FILE_PATH)\n",
    "daily_segments = segment_data_by_day(data_df)\n",
    "# Use the first 10 days\n",
    "# daily_segments = daily_segments[:10]\n",
    "# Find the average length of the segments\n",
    "print(len(daily_segments))\n",
    "avg_length = 0\n",
    "for segment in daily_segments:\n",
    "    avg_length += len(segment)\n",
    "avg_length /= len(daily_segments)\n",
    "avg_length = int(avg_length)\n",
    "print(avg_length)\n",
    "window_size = avg_length\n",
    "overlap_ratio = 0.2\n",
    "windows = sliding_window(daily_segments, window_size=window_size, overlap_ratio=overlap_ratio)\n",
    "print(len(windows))\n",
    "# Prepare the data\n",
    "window_labels = []\n",
    "windows = np.asarray([window.to_numpy() for window in windows])\n",
    "for window in windows:\n",
    "    activity_label = window[0][4]\n",
    "    activity_status_label = window[0][5]\n",
    "    label = (activity_label, activity_status_label)\n",
    "    window_labels.append(label)\n",
    "# Split the data into training and testing sets\n",
    "batch_size = 16  \n",
    "validation_split = 0.3\n",
    "timesteps = window_size\n",
    "input_dim = windows[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new train-test split using the windows\n",
    "window_train, window_val = train_test_split(windows, test_size=validation_split, shuffle=False)\n",
    "\n",
    "# Prepare the input data for the model by concatenating the windows along the time axis\n",
    "X_train = np.concatenate(window_train, axis=0)\n",
    "X_val = np.concatenate(window_val, axis=0)\n",
    "\n",
    "# Normalize the data using minMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train = X_train.reshape((-1, timesteps, input_dim))\n",
    "X_val = X_val.reshape((-1, timesteps, input_dim))\n",
    "\n",
    "latent_dim = 2\n",
    "encoding_dim = 32\n",
    "\n",
    "# ==================== ENCODER ====================\n",
    "inputs = Input(shape=(timesteps, input_dim), name='encoder_input')\n",
    "# print('encoder input shape: ', inputs.shape)\n",
    "x = Bidirectional(LSTM(encoding_dim * 2, return_sequences=True))(inputs)\n",
    "# print('first encoder bidirectional lstm shape: ', x.shape)\n",
    "x = Bidirectional(LSTM(encoding_dim, return_sequences=False))(x)\n",
    "# print('second encoder bidirectional lstm shape: ', x.shape)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "# print('encoder z_mean shape: ', z_mean.shape)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "# print('encoder z_log_var shape: ', z_log_var.shape)\n",
    "# z_mean is the mean of the latent space\n",
    "# z_log_var is the variance of the latent space\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "# print('encoder z shape: ', z.shape)\n",
    "# ================= LATENT SPACE ==================\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "# print('latent space input shape: ', latent_inputs.shape)\n",
    "# ==================== DECODER ====================\n",
    "x = Dense(timesteps * encoding_dim, activation='relu')(latent_inputs)\n",
    "# print('decoder dense shape: ', x.shape)\n",
    "x = Reshape((timesteps, encoding_dim))(x)\n",
    "# print('decoder reshape shape: ', x.shape)\n",
    "x = Bidirectional(LSTM(encoding_dim, return_sequences=True, input_shape=(timesteps, encoding_dim)))(x)\n",
    "# print('decoder bidirectional lstm shape: ', x.shape)\n",
    "x = TimeDistributed(Dense(input_dim))(x)\n",
    "# print('decoder time distributed dense shape: ', x.shape)\n",
    "# LSTM layer in the decoder is used to reconstruct the original sequence\n",
    "# print the summary\n",
    "\n",
    "# the VAE model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "# print('encoder model shape: ', encoder.output_shape)\n",
    "decoder = Model(latent_inputs, x, name='decoder')\n",
    "# print('decoder model shape: ', decoder.output_shape)\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "# print('outputs shape: ', outputs.shape)\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "# print('vae model shape: ', vae.output_shape)\n",
    "# print the summaries\n",
    "# encoder.summary()\n",
    "# decoder.summary()\n",
    "# vae.summary()\n",
    "\n",
    "\n",
    "# VAE loss function with custom_penalty\n",
    "reconstruction_loss = K.mean(K.square(inputs - outputs))\n",
    "reconstruction_loss *= timesteps * input_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "\n",
    "# Add the custom penalty to the loss function\n",
    "penalty_weight = 10.0  # Adjust the weight of the penalty term as needed\n",
    "penalty_layer = CustomPenaltyLayer(scaler, input_dim, timesteps)\n",
    "penalty = penalty_layer(outputs)\n",
    "penalty *= penalty_weight\n",
    "\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss + penalty)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "# vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 250\n",
    "history = vae.fit(X_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, None))\n",
    "\n",
    "# Use the encoder to generate embeddings for each sequence\n",
    "encoder_model = Model(inputs, z_mean)\n",
    "X_embedded = encoder_model.predict(X_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fake dataset using the VAE model\n",
    "n_samples = len(windows)\n",
    "\n",
    "# Sample from the latent space\n",
    "z_samples = np.random.normal(size=(n_samples, latent_dim))\n",
    "\n",
    "# Use the decoder to generate the output\n",
    "predicted_values = decoder.predict(z_samples)\n",
    "predicted_values = np.reshape(predicted_values, (n_samples, timesteps, input_dim))\n",
    "\n",
    "# Undo the normalization\n",
    "predicted_values = np.reshape(predicted_values, (-1, input_dim))\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Create the fake dataset in the original format\n",
    "fake_dataset = []\n",
    "for window in predicted_values.reshape((n_samples, timesteps, input_dim)):\n",
    "    fake_dataset.extend(window)\n",
    "\n",
    "# Save the fake dataset to a new file 'fake_dataset.txt'\n",
    "fake_data = pd.DataFrame(fake_dataset, columns=['Date', 'Time', 'Device ID', 'Status', 'Activity', 'Activity Status'])\n",
    "\n",
    "with open('../Predictions/Aruba_17_prediction_OLD.txt', 'w') as file:\n",
    "    for _, row in fake_data.iterrows():\n",
    "        file.write(','.join(map(str, row.values)) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = np.array(window_labels)\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    weighted_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    weighted_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return weighted_f1, weighted_precision, weighted_recall, balanced_accuracy\n",
    "\n",
    "# Split the embeddings and labels into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_embedded, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_val)\n",
    "\n",
    "weighted_f1, weighted_precision, weighted_recall, balanced_accuracy = evaluate_model(y_val, y_pred)\n",
    "\n",
    "print(\"Weighted F1-score:\", weighted_f1)\n",
    "print(\"Weighted Precision:\", weighted_precision)\n",
    "print(\"Weighted Recall:\", weighted_recall)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.-1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
