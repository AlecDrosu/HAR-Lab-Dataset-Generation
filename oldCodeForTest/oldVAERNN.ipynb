{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7921\n",
      "7921\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, TimeDistributed, Reshape, Bidirectional\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from oldslidingWindow import read_data, segment_data_by_day, sliding_window\n",
    "from oldCustom_layers import CustomPenaltyLayer\n",
    "FILE_PATH = '../Processed Data/Aruba_17/processed_data.csv'\n",
    "\n",
    "data_df = read_data(FILE_PATH)\n",
    "daily_segments = segment_data_by_day(data_df)\n",
    "# Use the first 10 days\n",
    "daily_segments = daily_segments[:100]\n",
    "# Find the average length of the segments\n",
    "avg_length = 0\n",
    "for segment in daily_segments:\n",
    "    avg_length += len(segment)\n",
    "avg_length /= len(daily_segments)\n",
    "avg_length = int(avg_length)\n",
    "print(avg_length)\n",
    "window_size = avg_length\n",
    "overlap_ratio = 0.5\n",
    "windows = sliding_window(daily_segments, window_size=window_size, overlap_ratio=overlap_ratio)\n",
    "\n",
    "# Prepare the data\n",
    "windows = np.asarray([window.to_numpy() for window in windows])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "timesteps = window_size\n",
    "print(timesteps)\n",
    "input_dim = windows[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 54s 21s/step - loss: 31521.2344 - val_loss: 36155.4766\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 38s 18s/step - loss: 32155.5742 - val_loss: 35255.3906\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 37s 18s/step - loss: 30358.3418 - val_loss: 34029.6680\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 28813.3477 - val_loss: 32356.7559\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 36s 17s/step - loss: 27002.0684 - val_loss: 30696.1152\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 36s 18s/step - loss: 24552.0352 - val_loss: 26231.7930\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 40s 19s/step - loss: 21144.7305 - val_loss: 22735.4141\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 16520.3047 - val_loss: 17333.1777\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 42s 19s/step - loss: 15189.7881 - val_loss: 13820.0293\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 11834.6055 - val_loss: 11282.0889\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 9893.6367 - val_loss: 8693.7119\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 7690.7329 - val_loss: 7070.1577\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 42s 21s/step - loss: 6154.3013 - val_loss: 5633.5615\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 5998.4697 - val_loss: 6194.7256\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 41s 20s/step - loss: 6127.5234 - val_loss: 6471.2891\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 38s 18s/step - loss: 5828.9858 - val_loss: 6826.2764\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 41s 20s/step - loss: 6243.8311 - val_loss: 6709.5781\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 41s 20s/step - loss: 6008.3066 - val_loss: 6701.6235\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 5892.7695 - val_loss: 6657.4443\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 40s 19s/step - loss: 5784.4062 - val_loss: 6552.7485\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 40s 19s/step - loss: 5146.2773 - val_loss: 6405.1294\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 42s 21s/step - loss: 5282.0537 - val_loss: 6470.3667\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 40s 18s/step - loss: 5630.4023 - val_loss: 6348.3003\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 5267.2900 - val_loss: 6191.1147\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 37s 18s/step - loss: 5106.0630 - val_loss: 6115.3633\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 5508.6357 - val_loss: 6013.7266\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 41s 20s/step - loss: 5482.2334 - val_loss: 5882.9180\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 5251.8867 - val_loss: 5798.6055\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 5316.5068 - val_loss: 5815.5127\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 5354.6235 - val_loss: 5733.0210\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 5190.2422 - val_loss: 5812.5781\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 5332.5278 - val_loss: 5702.1152\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 4955.8096 - val_loss: 5707.9355\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 38s 18s/step - loss: 5269.3218 - val_loss: 5868.6025\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 39s 18s/step - loss: 5105.3633 - val_loss: 5992.1768\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 4794.9497 - val_loss: 6178.6724\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 40s 19s/step - loss: 4908.9678 - val_loss: 6513.2373\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 37s 18s/step - loss: 5361.9370 - val_loss: 6687.6040\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 5377.2354 - val_loss: 6632.4717\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 38s 18s/step - loss: 5400.2080 - val_loss: 6373.9775\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 4905.3369 - val_loss: 6148.9419\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 38s 18s/step - loss: 4963.2378 - val_loss: 6047.0718\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 4879.6177 - val_loss: 6058.4990\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 4917.7012 - val_loss: 6185.1226\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 37s 19s/step - loss: 5301.6357 - val_loss: 6291.1030\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 5478.4624 - val_loss: 6195.7749\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 39s 20s/step - loss: 5131.0605 - val_loss: 5973.4194\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 39s 18s/step - loss: 5055.4233 - val_loss: 5817.1514\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 37s 18s/step - loss: 4924.0747 - val_loss: 5818.8013\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 37s 18s/step - loss: 5227.1572 - val_loss: 5878.4688\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 5037.0742 - val_loss: 5891.1880\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 39s 20s/step - loss: 5014.3057 - val_loss: 5897.9912\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 5550.4717 - val_loss: 5835.2158\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 41s 21s/step - loss: 4899.5605 - val_loss: 5665.6436\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 45s 21s/step - loss: 4905.5439 - val_loss: 5705.2998\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 45s 19s/step - loss: 4907.7051 - val_loss: 5940.2275\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 40s 19s/step - loss: 4898.6040 - val_loss: 6228.9272\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 5064.1943 - val_loss: 6503.8052\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 37s 19s/step - loss: 5243.9170 - val_loss: 6578.2969\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 37s 18s/step - loss: 5094.7466 - val_loss: 6481.5991\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 5523.1157 - val_loss: 6272.1528\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 5262.8560 - val_loss: 5929.6040\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 38s 18s/step - loss: 4893.8174 - val_loss: 5653.3193\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 39s 18s/step - loss: 5204.4653 - val_loss: 5539.0586\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 39s 20s/step - loss: 5155.4238 - val_loss: 5377.3804\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 5153.5776 - val_loss: 5299.4819\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 41s 20s/step - loss: 5167.6133 - val_loss: 5235.3604\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 38s 18s/step - loss: 5034.8042 - val_loss: 5253.3359\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 4952.6934 - val_loss: 5475.6030\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 5025.0635 - val_loss: 5776.1553\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 5043.2441 - val_loss: 6090.9824\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 5233.6201 - val_loss: 6260.1602\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 5032.2798 - val_loss: 6330.0029\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 5254.0259 - val_loss: 6253.5142\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 40s 19s/step - loss: 5042.5840 - val_loss: 6069.2720\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 40s 19s/step - loss: 5179.9580 - val_loss: 5893.8818\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 5162.9121 - val_loss: 5629.0015\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 5149.3105 - val_loss: 5371.0200\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 41s 21s/step - loss: 4993.9971 - val_loss: 5196.1250\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 38s 18s/step - loss: 5040.6841 - val_loss: 5168.6753\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 4820.7632 - val_loss: 5327.9375\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 5167.2402 - val_loss: 5622.9023\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 4863.9893 - val_loss: 5854.7471\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 40s 21s/step - loss: 4993.2275 - val_loss: 6077.8237\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 4859.9824 - val_loss: 6208.6772\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 4837.2139 - val_loss: 6352.0791\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 39s 20s/step - loss: 4804.6680 - val_loss: 6428.1953\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 4996.5835 - val_loss: 6433.2393\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 42s 21s/step - loss: 4986.8706 - val_loss: 6370.2524\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 38s 18s/step - loss: 4891.5029 - val_loss: 6045.4585\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 38s 18s/step - loss: 4712.3438 - val_loss: 5816.8110\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 38s 18s/step - loss: 4896.8232 - val_loss: 5506.1665\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 4730.9463 - val_loss: 5254.7969\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 37s 18s/step - loss: 4728.5386 - val_loss: 5235.7847\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 39s 19s/step - loss: 4982.6787 - val_loss: 5074.7256\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 41s 20s/step - loss: 4902.9121 - val_loss: 4785.1406\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 40s 20s/step - loss: 4625.2456 - val_loss: 4471.5093\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 4932.5664 - val_loss: 4041.5347\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 4519.2705 - val_loss: 3583.3308\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 38s 19s/step - loss: 4338.5142 - val_loss: 3177.2483\n",
      "2/2 [==============================] - 4s 978ms/step\n"
     ]
    }
   ],
   "source": [
    "# Create a new train-test split using the windows\n",
    "window_train, window_val = train_test_split(windows, test_size=validation_split, shuffle=False)\n",
    "\n",
    "# Prepare the input data for the model by concatenating the windows along the time axis\n",
    "X_train = np.concatenate(window_train, axis=0)\n",
    "X_val = np.concatenate(window_val, axis=0)\n",
    "\n",
    "# Normalize the data using minMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train = X_train.reshape((-1, timesteps, input_dim))\n",
    "X_val = X_val.reshape((-1, timesteps, input_dim))\n",
    "\n",
    "latent_dim = 2\n",
    "encoding_dim = 32\n",
    "\n",
    "# ==================== ENCODER ====================\n",
    "inputs = Input(shape=(timesteps, input_dim), name='encoder_input')\n",
    "x = Bidirectional(LSTM(encoding_dim * 2, return_sequences=True))(inputs)\n",
    "x = Bidirectional(LSTM(encoding_dim, return_sequences=False))(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "# z_mean is the mean of the latent space\n",
    "# z_log_var is the variance of the latent space\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# ================= LATENT SPACE ==================\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "# ==================== DECODER ====================\n",
    "x = Dense(timesteps * encoding_dim, activation='relu')(latent_inputs)\n",
    "x = Reshape((timesteps, encoding_dim))(x)\n",
    "x = Bidirectional(LSTM(encoding_dim, return_sequences=True, input_shape=(timesteps, encoding_dim)))(x)\n",
    "x = TimeDistributed(Dense(input_dim))(x)\n",
    "# LSTM layer in the decoder is used to reconstruct the original sequence\n",
    "\n",
    "# the VAE model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = Model(latent_inputs, x, name='decoder')\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "# VAE loss function with custom_penalty\n",
    "reconstruction_loss = K.mean(K.square(inputs - outputs))\n",
    "reconstruction_loss *= timesteps * input_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "\n",
    "# Add the custom penalty to the loss function\n",
    "penalty_weight = 10.0  # Adjust the weight of the penalty term as needed\n",
    "penalty_layer = CustomPenaltyLayer(scaler, input_dim, timesteps)\n",
    "penalty = penalty_layer(outputs)\n",
    "penalty *= penalty_weight\n",
    "\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss + penalty)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "num_epochs = 100\n",
    "history = vae.fit(X_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, None))\n",
    "\n",
    "# Use the encoder to generate embeddings for each sequence\n",
    "encoder_model = Model(inputs, z_mean)\n",
    "\n",
    "# Use the encoder to generate embeddings for each sequence\n",
    "encoder_model = Model(inputs, z_mean)\n",
    "X_embedded = encoder_model.predict(X_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 450ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate a fake dataset using the VAE model\n",
    "n_samples = len(windows)\n",
    "\n",
    "# Sample from the latent space\n",
    "z_samples = np.random.normal(size=(n_samples, latent_dim))\n",
    "\n",
    "# Use the decoder to generate the output\n",
    "predicted_values = decoder.predict(z_samples)\n",
    "predicted_values = np.reshape(predicted_values, (n_samples, timesteps, input_dim))\n",
    "\n",
    "# Undo the normalization\n",
    "predicted_values = np.reshape(predicted_values, (-1, input_dim))\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Create the fake dataset in the original format\n",
    "fake_dataset = []\n",
    "for window in predicted_values.reshape((n_samples, timesteps, input_dim)):\n",
    "    fake_dataset.extend(window)\n",
    "\n",
    "# Save the fake dataset to a new file 'fake_dataset.txt'\n",
    "fake_data = pd.DataFrame(fake_dataset, columns=['Date', 'Time', 'Device ID', 'Status', 'Activity', 'Activity Status'])\n",
    "\n",
    "with open('Aruba_17_prediction.txt', 'w') as file:\n",
    "    for _, row in fake_data.iterrows():\n",
    "        file.write(','.join(map(str, row.values)) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
