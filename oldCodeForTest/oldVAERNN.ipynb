{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID Mapping: {'D001': 0, 'D002': 1, 'D004': 2, 'M001': 3, 'M002': 4, 'M003': 5, 'M004': 6, 'M005': 7, 'M006': 8, 'M007': 9, 'M008': 10, 'M009': 11, 'M010': 12, 'M011': 13, 'M012': 14, 'M013': 15, 'M014': 16, 'M015': 17, 'M016': 18, 'M017': 19, 'M018': 20, 'M019': 21, 'M020': 22, 'M021': 23, 'M022': 24, 'M023': 25, 'M024': 26, 'M025': 27, 'M026': 28, 'M027': 29, 'M028': 30, 'M029': 31, 'M030': 32, 'M031': 33, 'T001': 34, 'T002': 35, 'T003': 36, 'T004': 37, 'T005': 38}\n",
      "Status Mapping: {'16': 0, '16.5': 1, '17': 2, '17.5': 3, '18': 4, '18.5': 5, '19': 6, '19.5': 7, '20': 8, '20.5': 9, '21': 10, '21.5': 11, '22': 12, '22.5': 13, '23': 14, '23.5': 15, '24': 16, '24.5': 17, '25': 18, '25.5': 19, '26': 20, '26.5': 21, '27': 22, '27.5': 23, '28': 24, '28.5': 25, '29': 26, '29.5': 27, '30': 28, '30.5': 29, '31': 30, '31.5': 31, '32': 32, '32.5': 33, '33': 34, '33.5': 35, '34': 36, '34.5': 37, '35': 38, '35.5': 39, '36': 40, '36.5': 41, '37': 42, '37.5': 43, '38': 44, '38.5': 45, '39': 46, '39.5': 47, '40.5': 48, '41.5': 49, '42': 50, '42.5': 51, '43': 52, 'CLOSE': 53, 'OFF': 54, 'ON': 55, 'OPEN': 56}\n",
      "Activity Mapping: {'Bed_to_Toilet': 0, 'Eating': 1, 'Enter_Home': 2, 'Housekeeping': 3, 'Leave_Home': 4, 'Meal_Preparation': 5, 'Relax': 6, 'Respirate': 7, 'Sleeping': 8, 'Wash_Dishes': 9, 'Work': 10, nan: 11}\n",
      "Activity Status Mapping: {'begin': 0, 'end': 1, nan: 2}\n",
      "32\n",
      "7044\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, TimeDistributed, Reshape, Bidirectional\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from oldslidingWindow import read_data, segment_data_by_day, sliding_window\n",
    "from PROCESSING import pre_processed_data, model_processing_code, model_post_processing\n",
    "from oldCustom_layers import CustomPenaltyLayer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RAW_DATA = '../Raw Data/Aruba_17/data'\n",
    "PRE_PROCESSED = '../Processed Data/Aruba_17/pre_processed_data.csv'\n",
    "FILE_PATH = '../Processed Data/Aruba_17/processed_data.csv'\n",
    "PREDICTIONS = '../Predictions/Aruba_17_prediction_419.txt'\n",
    "COMPLETE_PREDICTION = '../Predictions/Aruba_17_completed_prediction_419.txt'\n",
    "pre_processed_data(RAW_DATA, PRE_PROCESSED)\n",
    "mappings = model_processing_code(PRE_PROCESSED, FILE_PATH)\n",
    "\n",
    "data_df = read_data(FILE_PATH)\n",
    "daily_segments = segment_data_by_day(data_df)\n",
    "# Use the first 10 days\n",
    "daily_segments = daily_segments[:32]\n",
    "# Find the average length of the segments\n",
    "print(len(daily_segments))\n",
    "avg_length = 0\n",
    "for segment in daily_segments:\n",
    "    avg_length += len(segment)\n",
    "avg_length /= len(daily_segments)\n",
    "avg_length = int(avg_length)\n",
    "print(avg_length)\n",
    "window_size = avg_length\n",
    "overlap_ratio = 0.2\n",
    "windows = sliding_window(daily_segments, window_size=window_size, overlap_ratio=overlap_ratio)\n",
    "print(len(windows))\n",
    "# Prepare the data\n",
    "window_labels = []\n",
    "windows = np.asarray([window.to_numpy() for window in windows])\n",
    "for window in windows:\n",
    "    activity_label = window[0][4]\n",
    "    activity_status_label = window[0][5]\n",
    "    label = (activity_label, activity_status_label)\n",
    "    window_labels.append(label)\n",
    "# Split the data into training and testing sets\n",
    "batch_size = 16  \n",
    "validation_split = 0.3\n",
    "timesteps = window_size\n",
    "input_dim = windows[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new train-test split using the windows\n",
    "window_train, window_val = train_test_split(windows, test_size=validation_split, shuffle=False)\n",
    "\n",
    "# Prepare the input data for the model by concatenating the windows along the time axis\n",
    "X_train = np.concatenate(window_train, axis=0)\n",
    "X_val = np.concatenate(window_val, axis=0)\n",
    "\n",
    "# Normalize the data using minMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train = X_train.reshape((-1, timesteps, input_dim))\n",
    "X_val = X_val.reshape((-1, timesteps, input_dim))\n",
    "\n",
    "latent_dim = 2\n",
    "encoding_dim = 32\n",
    "\n",
    "# ==================== ENCODER ====================\n",
    "inputs = Input(shape=(timesteps, input_dim), name='encoder_input')\n",
    "# print('encoder input shape: ', inputs.shape)\n",
    "x = Bidirectional(LSTM(encoding_dim * 2, return_sequences=True))(inputs)\n",
    "# print('first encoder bidirectional lstm shape: ', x.shape)\n",
    "x = Bidirectional(LSTM(encoding_dim, return_sequences=False))(x)\n",
    "# print('second encoder bidirectional lstm shape: ', x.shape)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "# print('encoder z_mean shape: ', z_mean.shape)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "# print('encoder z_log_var shape: ', z_log_var.shape)\n",
    "# z_mean is the mean of the latent space\n",
    "# z_log_var is the variance of the latent space\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "# print('encoder z shape: ', z.shape)\n",
    "# ================= LATENT SPACE ==================\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "# print('latent space input shape: ', latent_inputs.shape)\n",
    "# ==================== DECODER ====================\n",
    "x = Dense(timesteps * encoding_dim, activation='relu')(latent_inputs)\n",
    "# print('decoder dense shape: ', x.shape)\n",
    "x = Reshape((timesteps, encoding_dim))(x)\n",
    "# print('decoder reshape shape: ', x.shape)\n",
    "x = Bidirectional(LSTM(encoding_dim, return_sequences=True, input_shape=(timesteps, encoding_dim)))(x)\n",
    "# print('decoder bidirectional lstm shape: ', x.shape)\n",
    "x = TimeDistributed(Dense(input_dim))(x)\n",
    "# print('decoder time distributed dense shape: ', x.shape)\n",
    "# LSTM layer in the decoder is used to reconstruct the original sequence\n",
    "# print the summary\n",
    "\n",
    "# the VAE model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "# print('encoder model shape: ', encoder.output_shape)\n",
    "decoder = Model(latent_inputs, x, name='decoder')\n",
    "# print('decoder model shape: ', decoder.output_shape)\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "# print('outputs shape: ', outputs.shape)\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "# print('vae model shape: ', vae.output_shape)\n",
    "# print the summaries\n",
    "# encoder.summary()\n",
    "# decoder.summary()\n",
    "# vae.summary()\n",
    "\n",
    "\n",
    "# VAE loss function with custom_penalty\n",
    "reconstruction_loss = K.mean(K.square(inputs - outputs))\n",
    "reconstruction_loss *= timesteps * input_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "\n",
    "# Add the custom penalty to the loss function\n",
    "penalty_weight = 10.0  # Adjust the weight of the penalty term as needed\n",
    "penalty_layer = CustomPenaltyLayer(scaler, input_dim, timesteps)\n",
    "penalty = penalty_layer(outputs)\n",
    "penalty *= penalty_weight\n",
    "\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss + penalty)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "# vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 33s 33s/step - loss: 28974.3652 - val_loss: 118591.0156\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 16s 16s/step - loss: 29927.0234 - val_loss: 118003.0000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 15s 15s/step - loss: 29580.0527 - val_loss: 117436.2656\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 16s 16s/step - loss: 29210.1895 - val_loss: 116709.2031\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 16s 16s/step - loss: 28765.4316 - val_loss: 115945.3438\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 28310.0801 - val_loss: 114962.5703\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 27678.1621 - val_loss: 113791.1094\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 27053.1914 - val_loss: 112789.8281\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 26333.2383 - val_loss: 111509.9609\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 25729.1738 - val_loss: 110649.5312\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = vae.fit(X_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, None))\n",
    "\n",
    "# Use the encoder to generate embeddings for each sequence\n",
    "encoder_model = Model(inputs, z_mean)\n",
    "X_embedded = encoder_model.predict(X_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "         Date          Time  Device ID  Status  Activity  Activity Status\n",
      "0  20101106.0  1.097294e+10        4.0     3.0       1.0              0.0\n",
      "1  20101106.0  1.123590e+10        4.0     4.0       1.0              0.0\n",
      "2  20101106.0  1.118745e+10        4.0     4.0       1.0              0.0\n",
      "3  20101106.0  1.119466e+10        4.0     4.0       1.0              0.0\n",
      "4  20101106.0  1.140708e+10        4.0     4.0       1.0              0.0\n"
     ]
    }
   ],
   "source": [
    "# Generate a fake dataset using the VAE model\n",
    "n_samples = len(windows)\n",
    "\n",
    "# Sample from the latent space\n",
    "z_samples = np.random.normal(size=(n_samples, latent_dim))\n",
    "\n",
    "# Use the decoder to generate the output\n",
    "predicted_values = decoder.predict(z_samples)\n",
    "predicted_values = np.reshape(predicted_values, (n_samples, timesteps, input_dim))\n",
    "\n",
    "# Undo the normalization\n",
    "predicted_values = np.reshape(predicted_values, (-1, input_dim))\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Create the fake dataset in the original format\n",
    "fake_dataset = []\n",
    "for window in predicted_values.reshape((n_samples, timesteps, input_dim)):\n",
    "    fake_dataset.extend(window)\n",
    "\n",
    "# Save the fake dataset to a new file 'fake_dataset.txt'\n",
    "fake_data = pd.DataFrame(fake_dataset, columns=['Date', 'Time', 'Device ID', 'Status', 'Activity', 'Activity Status'])\n",
    "\n",
    "with open(PREDICTIONS, 'w') as file:\n",
    "    for _, row in fake_data.iterrows():\n",
    "        file.write(','.join(map(str, row.values)) + '\\n')\n",
    "\n",
    "model_post_processing(PREDICTIONS, COMPLETE_PREDICTION, mappings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = np.array(window_labels)\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    weighted_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    weighted_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return weighted_f1, weighted_precision, weighted_recall, balanced_accuracy\n",
    "\n",
    "# Split the embeddings and labels into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_embedded, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_val)\n",
    "\n",
    "weighted_f1, weighted_precision, weighted_recall, balanced_accuracy = evaluate_model(y_val, y_pred)\n",
    "\n",
    "print(\"Weighted F1-score:\", weighted_f1)\n",
    "print(\"Weighted Precision:\", weighted_precision)\n",
    "print(\"Weighted Recall:\", weighted_recall)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
