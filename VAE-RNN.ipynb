{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, RepeatVector, TimeDistributed, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the original dataset\n",
    "processed_data = pd.read_csv('Processed Data/Aruba_17/processed_data.csv')\n",
    "# Find the maximum number that can be evenly divisible by 32, given the length of the dataset\n",
    "max_length = len(processed_data) - len(processed_data) % 32\n",
    "processed_data = processed_data.head(6400)\n",
    "\n",
    "# Extract the relevant columns from the dataset\n",
    "timestamp = processed_data['Timestamp'].values\n",
    "device_id = processed_data['Device ID'].values\n",
    "status = processed_data['Status'].values\n",
    "activity = processed_data['Activity'].values\n",
    "activity_status = processed_data['Activity Status'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.stack((timestamp, device_id, status, activity, activity_status), axis=1)\n",
    "\n",
    "# # Normalize the data using z-score normalization\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "# # Scale the values to be within the range of 0 to 1\n",
    "# min_max_scaler = MinMaxScaler()\n",
    "# X = min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 9s 2s/step - loss: 2726.1409 - val_loss: 1210.1655\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 1685.4363 - val_loss: 1090.5575\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 1486.0469 - val_loss: 1038.2996\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 1399.1285 - val_loss: 1005.3638\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 1324.5244 - val_loss: 969.8781\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 1245.3683 - val_loss: 923.6373\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 1181.1282 - val_loss: 912.9176\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 1117.2798 - val_loss: 864.8527\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 1051.2363 - val_loss: 840.8665\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 1006.1376 - val_loss: 822.0356\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 940.2611 - val_loss: 823.6378\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 898.4077 - val_loss: 789.4713\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 804.5440 - val_loss: 752.7933\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 774.6682 - val_loss: 722.7216\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 711.5331 - val_loss: 703.2811\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 685.2809 - val_loss: 714.4572\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 695.0917 - val_loss: 724.8878\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 720.5285 - val_loss: 734.4381\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 716.3674 - val_loss: 716.0521\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 685.8945 - val_loss: 684.7148\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 665.1141 - val_loss: 694.3434\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 673.7449 - val_loss: 691.6564\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 671.0311 - val_loss: 687.6016\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 685.1270 - val_loss: 696.0485\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 680.0719 - val_loss: 691.2716\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 688.2086 - val_loss: 693.5869\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 680.4198 - val_loss: 692.5953\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 682.9101 - val_loss: 680.0955\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 682.7480 - val_loss: 677.0818\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 672.0328 - val_loss: 686.0083\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 670.2023 - val_loss: 676.0071\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 665.6449 - val_loss: 671.0834\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 662.8477 - val_loss: 664.3965\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 648.4414 - val_loss: 665.5748\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 649.0094 - val_loss: 663.9791\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 645.0798 - val_loss: 659.5054\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 638.5318 - val_loss: 658.9169\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 635.4452 - val_loss: 655.0892\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 636.1556 - val_loss: 657.6283\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 641.1521 - val_loss: 656.1619\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 633.5935 - val_loss: 654.3018\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 640.1832 - val_loss: 666.1671\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 633.8689 - val_loss: 654.4611\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 639.7906 - val_loss: 651.2771\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 633.3711 - val_loss: 648.9097\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 632.0107 - val_loss: 649.8377\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 632.7178 - val_loss: 644.8504\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 625.7992 - val_loss: 647.0577\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 630.4860 - val_loss: 643.2099\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 628.5972 - val_loss: 658.7886\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 629.6093 - val_loss: 650.7711\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 630.1298 - val_loss: 649.0910\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 637.0985 - val_loss: 652.4139\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 638.7327 - val_loss: 654.6446\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 644.5468 - val_loss: 652.0243\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 640.6394 - val_loss: 649.0355\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 641.7972 - val_loss: 645.8257\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 640.1280 - val_loss: 654.4978\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 638.5452 - val_loss: 645.5732\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 637.0529 - val_loss: 646.1538\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 633.1416 - val_loss: 648.6439\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 630.2869 - val_loss: 640.1770\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 625.6876 - val_loss: 638.1161\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 622.9271 - val_loss: 638.2054\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 624.1224 - val_loss: 638.7262\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 617.3513 - val_loss: 636.4293\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 619.4689 - val_loss: 633.5980\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 615.9138 - val_loss: 631.7933\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 616.9661 - val_loss: 631.7609\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 620.3076 - val_loss: 634.8833\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 614.0555 - val_loss: 628.8970\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 613.7213 - val_loss: 630.8447\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 617.6628 - val_loss: 630.0815\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 617.6115 - val_loss: 626.4102\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 611.0714 - val_loss: 627.3482\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 610.9131 - val_loss: 629.3622\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 611.2421 - val_loss: 630.2552\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 608.6044 - val_loss: 625.8634\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 610.1608 - val_loss: 624.0746\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 608.0359 - val_loss: 623.8677\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 609.3439 - val_loss: 626.4284\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 609.5933 - val_loss: 623.5872\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 608.1484 - val_loss: 621.8730\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 610.2297 - val_loss: 623.0085\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 607.6846 - val_loss: 621.3659\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 608.9835 - val_loss: 620.4540\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 613.0616 - val_loss: 619.6251\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 608.0137 - val_loss: 621.6569\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 606.0648 - val_loss: 619.3831\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 607.4552 - val_loss: 618.7871\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 605.5739 - val_loss: 618.0946\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 606.4285 - val_loss: 618.3542\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 605.6851 - val_loss: 617.5621\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 605.7654 - val_loss: 617.2763\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 603.5492 - val_loss: 616.4810\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 604.1442 - val_loss: 616.1392\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 605.1275 - val_loss: 615.0161\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 601.8772 - val_loss: 617.2867\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 602.6500 - val_loss: 617.1052\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 174ms/step - loss: 600.7269 - val_loss: 616.6923\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 599.1541 - val_loss: 615.3766\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 599.7703 - val_loss: 616.0002\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 599.1167 - val_loss: 615.4120\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 602.1506 - val_loss: 614.5746\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 599.4327 - val_loss: 615.7628\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 596.9662 - val_loss: 611.1010\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 601.0327 - val_loss: 614.7318\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 601.2623 - val_loss: 612.0032\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 602.1440 - val_loss: 611.4115\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 600.7231 - val_loss: 612.3892\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 601.0231 - val_loss: 610.3923\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 599.0513 - val_loss: 608.7470\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 601.1100 - val_loss: 609.6929\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 600.1377 - val_loss: 609.5191\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 600.0687 - val_loss: 610.5514\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 598.7718 - val_loss: 608.2704\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 598.3804 - val_loss: 608.2596\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 598.8707 - val_loss: 607.3840\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 598.9980 - val_loss: 609.4899\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 598.6508 - val_loss: 608.3868\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 597.2373 - val_loss: 607.2117\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 597.8418 - val_loss: 607.0109\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 597.6379 - val_loss: 607.3087\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 596.4619 - val_loss: 607.3325\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 596.1580 - val_loss: 606.4444\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 596.7922 - val_loss: 606.7622\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 597.1657 - val_loss: 605.3002\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 595.7388 - val_loss: 605.1969\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 596.0702 - val_loss: 605.6823\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 596.4026 - val_loss: 607.1855\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 596.3382 - val_loss: 605.7330\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 595.1000 - val_loss: 604.9818\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 595.6995 - val_loss: 604.7460\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 595.1924 - val_loss: 604.7865\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 594.7081 - val_loss: 606.0123\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 592.5872 - val_loss: 605.6377\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 593.2759 - val_loss: 605.0259\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 592.0187 - val_loss: 605.1838\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 592.1261 - val_loss: 612.8883\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 598.5567 - val_loss: 616.4282\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 617.7585 - val_loss: 626.0690\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 629.0677 - val_loss: 631.0794\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 638.5167 - val_loss: 633.5342\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 647.8022 - val_loss: 635.7275\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 654.6554 - val_loss: 641.4569\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 656.6151 - val_loss: 636.2476\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 656.6351 - val_loss: 635.7776\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 654.6189 - val_loss: 632.2522\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 651.9238 - val_loss: 631.4060\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 647.2830 - val_loss: 628.3107\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 643.3629 - val_loss: 625.6942\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 637.7349 - val_loss: 623.2029\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 632.8040 - val_loss: 620.5502\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 627.7689 - val_loss: 618.8525\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 623.3736 - val_loss: 615.7460\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 618.6245 - val_loss: 613.5707\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 615.9982 - val_loss: 611.1668\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 610.5234 - val_loss: 610.9280\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 607.7676 - val_loss: 607.6044\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 602.9644 - val_loss: 608.5016\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 600.8088 - val_loss: 606.8106\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 598.5284 - val_loss: 604.5562\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 595.6088 - val_loss: 604.3272\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 593.2878 - val_loss: 604.1731\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 593.3386 - val_loss: 601.9995\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 591.3419 - val_loss: 602.2364\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 589.8638 - val_loss: 603.2313\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 589.7278 - val_loss: 602.5965\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 589.1667 - val_loss: 603.1132\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 589.2830 - val_loss: 601.6948\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 589.1208 - val_loss: 601.3381\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 589.5877 - val_loss: 601.7302\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 588.6097 - val_loss: 601.5834\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 588.5238 - val_loss: 601.6047\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 588.1163 - val_loss: 601.0299\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 587.1592 - val_loss: 602.2397\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 587.2455 - val_loss: 601.2686\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 0s 184ms/step - loss: 587.4415 - val_loss: 602.3461\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 588.0754 - val_loss: 600.4517\n",
      "Epoch 180/300\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 588.2855 - val_loss: 599.8705\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 589.0875 - val_loss: 600.1550\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 588.1989 - val_loss: 601.9487\n",
      "Epoch 183/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 585.4655 - val_loss: 604.7335\n",
      "Epoch 184/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 587.9647 - val_loss: 603.9654\n",
      "Epoch 185/300\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 588.0234 - val_loss: 603.2719\n",
      "Epoch 186/300\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 589.1605 - val_loss: 601.7718\n",
      "Epoch 187/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 586.1824 - val_loss: 600.5974\n",
      "Epoch 188/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 586.2377 - val_loss: 602.6100\n",
      "Epoch 189/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 588.0134 - val_loss: 599.6851\n",
      "Epoch 190/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 589.7388 - val_loss: 599.8076\n",
      "Epoch 191/300\n",
      "2/2 [==============================] - 0s 171ms/step - loss: 590.4260 - val_loss: 599.5724\n",
      "Epoch 192/300\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 589.9695 - val_loss: 600.0067\n",
      "Epoch 193/300\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 590.6056 - val_loss: 599.7405\n",
      "Epoch 194/300\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 590.5807 - val_loss: 599.6810\n",
      "Epoch 195/300\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 588.7968 - val_loss: 603.9019\n",
      "Epoch 196/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 589.8049 - val_loss: 600.4368\n",
      "Epoch 197/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 585.8416 - val_loss: 601.0052\n",
      "Epoch 198/300\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 586.2630 - val_loss: 603.1811\n",
      "Epoch 199/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 588.1454 - val_loss: 604.4688\n",
      "Epoch 200/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 586.1234 - val_loss: 602.0900\n",
      "Epoch 201/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 587.1998 - val_loss: 603.1074\n",
      "Epoch 202/300\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 584.5718 - val_loss: 600.7772\n",
      "Epoch 203/300\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 584.5933 - val_loss: 601.5031\n",
      "Epoch 204/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 585.5850 - val_loss: 602.3649\n",
      "Epoch 205/300\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 584.8821 - val_loss: 600.8821\n",
      "Epoch 206/300\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 584.7173 - val_loss: 600.5314\n",
      "Epoch 207/300\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 584.2091 - val_loss: 599.7013\n",
      "Epoch 208/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 585.7144 - val_loss: 600.0611\n",
      "Epoch 209/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 586.3503 - val_loss: 600.2246\n",
      "Epoch 210/300\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 587.7745 - val_loss: 598.8437\n",
      "Epoch 211/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 585.1610 - val_loss: 598.4575\n",
      "Epoch 212/300\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 585.1981 - val_loss: 599.4648\n",
      "Epoch 213/300\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 584.9365 - val_loss: 599.3259\n",
      "Epoch 214/300\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 586.5712 - val_loss: 598.6835\n",
      "Epoch 215/300\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 586.9006 - val_loss: 598.5872\n",
      "Epoch 216/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 587.0958 - val_loss: 598.7064\n",
      "Epoch 217/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 585.4119 - val_loss: 600.4453\n",
      "Epoch 218/300\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 586.9574 - val_loss: 605.2845\n",
      "Epoch 219/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 586.8025 - val_loss: 603.2078\n",
      "Epoch 220/300\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 588.4401 - val_loss: 603.1077\n",
      "Epoch 221/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 588.8892 - val_loss: 602.9050\n",
      "Epoch 222/300\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 584.6333 - val_loss: 602.9196\n",
      "Epoch 223/300\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 585.3665 - val_loss: 600.6199\n",
      "Epoch 224/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 584.8184 - val_loss: 599.3149\n",
      "Epoch 225/300\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 587.0440 - val_loss: 597.8975\n",
      "Epoch 226/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 586.0358 - val_loss: 598.0203\n",
      "Epoch 227/300\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 586.3280 - val_loss: 600.4187\n",
      "Epoch 228/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 585.5726 - val_loss: 600.9215\n",
      "Epoch 229/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 584.3395 - val_loss: 603.8175\n",
      "Epoch 230/300\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 585.8374 - val_loss: 602.4852\n",
      "Epoch 231/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 586.5219 - val_loss: 602.2505\n",
      "Epoch 232/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 585.7943 - val_loss: 601.6160\n",
      "Epoch 233/300\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 585.1431 - val_loss: 600.5334\n",
      "Epoch 234/300\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 583.9238 - val_loss: 599.9183\n",
      "Epoch 235/300\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 584.9426 - val_loss: 599.7592\n",
      "Epoch 236/300\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 584.7623 - val_loss: 598.8824\n",
      "Epoch 237/300\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 585.0231 - val_loss: 598.6234\n",
      "Epoch 238/300\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 587.1376 - val_loss: 598.2562\n",
      "Epoch 239/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 586.1151 - val_loss: 598.0013\n",
      "Epoch 240/300\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 585.7856 - val_loss: 598.9590\n",
      "Epoch 241/300\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 584.6221 - val_loss: 597.9799\n",
      "Epoch 242/300\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 586.2522 - val_loss: 599.0923\n",
      "Epoch 243/300\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 585.5211 - val_loss: 600.0043\n",
      "Epoch 244/300\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 585.8911 - val_loss: 598.4256\n",
      "Epoch 245/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 586.5742 - val_loss: 599.6943\n",
      "Epoch 246/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 584.8071 - val_loss: 598.6970\n",
      "Epoch 247/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 583.9818 - val_loss: 598.8457\n",
      "Epoch 248/300\n",
      "2/2 [==============================] - 0s 233ms/step - loss: 583.3065 - val_loss: 598.4821\n",
      "Epoch 249/300\n",
      "2/2 [==============================] - 0s 167ms/step - loss: 583.4105 - val_loss: 598.7262\n",
      "Epoch 250/300\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 585.1564 - val_loss: 599.1043\n",
      "Epoch 251/300\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 583.7330 - val_loss: 598.6211\n",
      "Epoch 252/300\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 584.2350 - val_loss: 598.2836\n",
      "Epoch 253/300\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 583.7283 - val_loss: 598.3092\n",
      "Epoch 254/300\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 583.0056 - val_loss: 598.6882\n",
      "Epoch 255/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 584.7900 - val_loss: 598.2723\n",
      "Epoch 256/300\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 582.9280 - val_loss: 599.0487\n",
      "Epoch 257/300\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 581.8403 - val_loss: 599.1633\n",
      "Epoch 258/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 586.0331 - val_loss: 598.6039\n",
      "Epoch 259/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 582.3344 - val_loss: 598.4683\n",
      "Epoch 260/300\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 584.9098 - val_loss: 599.1802\n",
      "Epoch 261/300\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 584.0378 - val_loss: 597.9623\n",
      "Epoch 262/300\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 583.3529 - val_loss: 597.7612\n",
      "Epoch 263/300\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 583.3404 - val_loss: 599.3165\n",
      "Epoch 264/300\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 582.1020 - val_loss: 598.1222\n",
      "Epoch 265/300\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 584.5114 - val_loss: 599.2293\n",
      "Epoch 266/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 582.1152 - val_loss: 598.9388\n",
      "Epoch 267/300\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 582.1059 - val_loss: 600.3817\n",
      "Epoch 268/300\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 582.6625 - val_loss: 599.4349\n",
      "Epoch 269/300\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 581.2789 - val_loss: 600.6838\n",
      "Epoch 270/300\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 581.3596 - val_loss: 598.7782\n",
      "Epoch 271/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 581.6375 - val_loss: 598.0719\n",
      "Epoch 272/300\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 580.5267 - val_loss: 598.7801\n",
      "Epoch 273/300\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 579.5510 - val_loss: 598.6304\n",
      "Epoch 274/300\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 580.4810 - val_loss: 597.5665\n",
      "Epoch 275/300\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 582.6509 - val_loss: 599.0511\n",
      "Epoch 276/300\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 583.1889 - val_loss: 597.8955\n",
      "Epoch 277/300\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 580.2625 - val_loss: 598.5686\n",
      "Epoch 278/300\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 579.7173 - val_loss: 598.5282\n",
      "Epoch 279/300\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 580.3583 - val_loss: 597.9687\n",
      "Epoch 280/300\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 579.9503 - val_loss: 597.9856\n",
      "Epoch 281/300\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 583.4090 - val_loss: 597.5345\n",
      "Epoch 282/300\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 579.7000 - val_loss: 599.1030\n",
      "Epoch 283/300\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 579.2265 - val_loss: 599.8927\n",
      "Epoch 284/300\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 579.9508 - val_loss: 599.0404\n",
      "Epoch 285/300\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 581.3914 - val_loss: 599.2170\n",
      "Epoch 286/300\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 580.1729 - val_loss: 600.1509\n",
      "Epoch 287/300\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 579.0757 - val_loss: 599.4767\n",
      "Epoch 288/300\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 579.3557 - val_loss: 601.5384\n",
      "Epoch 289/300\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 581.0941 - val_loss: 600.7974\n",
      "Epoch 290/300\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 580.2805 - val_loss: 600.2437\n",
      "Epoch 291/300\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 579.8021 - val_loss: 599.2477\n",
      "Epoch 292/300\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 582.4009 - val_loss: 599.0499\n",
      "Epoch 293/300\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 579.6372 - val_loss: 597.6049\n",
      "Epoch 294/300\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 582.6333 - val_loss: 597.5821\n",
      "Epoch 295/300\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 580.5569 - val_loss: 597.7007\n",
      "Epoch 296/300\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 584.0579 - val_loss: 596.9696\n",
      "Epoch 297/300\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 584.0199 - val_loss: 597.2075\n",
      "Epoch 298/300\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 581.5974 - val_loss: 598.1999\n",
      "Epoch 299/300\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 579.6215 - val_loss: 598.1139\n",
      "Epoch 300/300\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 580.0134 - val_loss: 602.4503\n",
      "2/2 [==============================] - 1s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the log directory for TensorBoard\n",
    "# log_dir = \"logs/\"\n",
    "\n",
    "# Create a callback for TensorBoard\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Prepare the data for input into the VAE model\n",
    "X = np.stack((timestamp, device_id, status, activity, activity_status), axis=1)\n",
    "\n",
    "# Normalize the data using minMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# print(X.head(20))\n",
    "\n",
    "# Use KMeans to cluster sequences into 14 different groups\n",
    "kmeans = KMeans(n_clusters=14, random_state=0)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "timesteps = 128 # number of previous records considered\n",
    "input_dim = X.shape[1] # number of features, there are 5 features in the dataset\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, clusters, test_size=validation_split, shuffle=False)\n",
    "\n",
    "# Pad the data to ensure it is divisible by the desired shape\n",
    "remainder_train = X_train.shape[0] % (batch_size * timesteps)\n",
    "if remainder_train > 0:\n",
    "    X_train = np.concatenate([X_train, np.zeros((batch_size * timesteps - remainder_train, input_dim))])\n",
    "    y_train = np.concatenate([y_train, np.zeros((batch_size * timesteps - remainder_train,))])\n",
    "    \n",
    "remainder_val = X_val.shape[0] % (batch_size * timesteps)\n",
    "if remainder_val > 0:\n",
    "    X_val = np.concatenate([X_val, np.zeros((batch_size * timesteps - remainder_val, input_dim))])\n",
    "    y_val = np.concatenate([y_val, np.zeros((batch_size * timesteps - remainder_val,))])\n",
    "\n",
    "# Reshape the datasets to have the correct shape for the model\n",
    "X_train = X_train.reshape((-1, timesteps, input_dim))\n",
    "y_train = y_train.reshape((-1, timesteps))\n",
    "\n",
    "X_val = X_val.reshape((-1, timesteps, input_dim))\n",
    "y_val = y_val.reshape((-1, timesteps))\n",
    "\n",
    "latent_dim = 2\n",
    "encoding_dim = 32\n",
    "\n",
    "# ==================== ENCODER ====================\n",
    "inputs = Input(batch_shape=(batch_size, timesteps, input_dim), name='encoder_input')\n",
    "x = LSTM(encoding_dim*2, return_sequences=True)(inputs)\n",
    "x = LSTM(encoding_dim, return_sequences=False)(x) \n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "# z_mean is the mean of the latent space\n",
    "# z_log_var is the variance of the latent space\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "# encoder.summary()\n",
    "\n",
    "# ================= LATENT SPACE ==================\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "# ==================== DECODER ====================\n",
    "x = Dense(timesteps * encoding_dim, activation='relu')(latent_inputs)\n",
    "x = Reshape((timesteps, encoding_dim))(x)\n",
    "x = LSTM(encoding_dim, return_sequences=True, input_shape=(timesteps, encoding_dim))(x)\n",
    "x = TimeDistributed(Dense(input_dim))(x)\n",
    "# LSTM layer in the decoder is used to reconstruct the original sequence\n",
    "\n",
    "# the VAE model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = Model(latent_inputs, x, name='decoder')\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "# Loss function\n",
    "# reconstruction_loss = K.mean(binary_crossentropy(K.flatten(inputs), K.flatten(outputs)))\n",
    "# reconstruction_loss *= timesteps * input_dim\n",
    "# kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "# kl_loss = K.mean(kl_loss, axis=-1)\n",
    "# kl_loss *= -0.5\n",
    "# # vae_loss = reconstruction_loss + kl_loss\n",
    "# vae_loss = reconstruction_loss + kl_loss\n",
    "# vae.add_loss(vae_loss)\n",
    "# vae.compile(optimizer='adam')\n",
    "reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n",
    "# reconstruction_loss = K.mean(reconstruction_loss)\n",
    "reconstruction_loss *= timesteps * input_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "# kl_loss = K.mean(kl_loss, axis=-1)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "# vae_loss = reconstruction_loss + kl_loss\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "# vae.summary()\n",
    "# callbacks=[tensorboard_callback]\n",
    "num_epochs = 300\n",
    "history = vae.fit(X_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
    "\n",
    "# plot_model(vae, to_file='model.png', show_shapes=True)\n",
    "# Use the encoder to generate embeddings for each sequence\n",
    "encoder_model = Model(inputs, z_mean)\n",
    "# print(encoder_model.layers[0].input_shape)\n",
    "\n",
    "X_embedded = encoder_model.predict(X_train, batch_size=batch_size)\n",
    "# Potentially change from the encoder_model to the vae_model, it will be slower but will be more expressive and representative of the data.\n",
    "\n",
    "# Train a classifier on the embeddings\n",
    "y_pred = kmeans.fit_predict(X_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 8s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate a fake dataset using the VAE model\n",
    "n_samples = len(processed_data)\n",
    "\n",
    "noise = np.random.normal(size=(n_samples, timesteps, input_dim))\n",
    "predicted_values = vae.predict(noise, batch_size=batch_size)\n",
    "# reshape predicted values to have the correct shape\n",
    "predicted_values = np.reshape(predicted_values, (n_samples, timesteps, input_dim))\n",
    "\n",
    "# undo the normalization\n",
    "predicted_values = np.reshape(predicted_values, (-1, input_dim))\n",
    "# predicted_values = min_max_scaler.inverse_transform(predicted_values)\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Save the prediction data to a new file 'predicted_Data.csv'\n",
    "predicted_data = pd.DataFrame(predicted_values.reshape((-1, input_dim)), columns=['Timestamp', 'Device ID', 'Status', 'Activity', 'Activity Status'])\n",
    "# predicted_data['Cluster'] = y_pred.reshape(-1)\n",
    "# predicted_data.to_csv('Predictions/Aruba_17_prediction.csv', index=False)\n",
    "with open('Predictions/Aruba_17_prediction.txt', 'w') as file:\n",
    "    for _, row in predicted_data.iterrows():\n",
    "        file.write(','.join(map(str, row.values)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation loss with x and y labels, and a grid\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "# Validation loss > training loss, underfitting\n",
    "# validation loss > training loss, overfitting, if it decreases and then increases again.\n",
    "# If they both decreease and stabilize at a specific point, it is an optimal fit.\n",
    "\n",
    "# Plot the evaluation loss vs the iterations\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the model\n",
    "# from keras.utils import plot_model\n",
    "\n",
    "# # Display the layers, number of layers, number of nodes etc\n",
    "# plot_model(vae, to_file='vae.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# # Load the image and display it\n",
    "# img = plt.imread('vae.png')\n",
    "# plt.figure(figsize=(16, 12))\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e3f0318fa44a63fbd15a81336d0e6b9929111f70e7cf4cecf151c11d26f00aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
