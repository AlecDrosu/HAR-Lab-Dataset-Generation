{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, RepeatVector, TimeDistributed, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "# from custom_penalty import custom_penalty\n",
    "\n",
    "# Load the original dataset\n",
    "processed_data = pd.read_csv('Processed Data/Aruba_17/processed_data.csv')\n",
    "# Find the maximum number that can be evenly divisible by 32, given the length of the dataset\n",
    "max_length = len(processed_data) - len(processed_data) % 32\n",
    "# processed_data = processed_data.head(16000)\n",
    "# only use all the rows up to the max_length\n",
    "processed_data = processed_data.head(3200)\n",
    "# Extract the relevant columns from the dataset\n",
    "timestamp = processed_data['Timestamp'].values\n",
    "device_id = processed_data['Device ID'].values\n",
    "status = processed_data['Status'].values\n",
    "activity = processed_data['Activity'].values\n",
    "activity_status = processed_data['Activity Status'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.stack((timestamp, device_id, status, activity, activity_status), axis=1)\n",
    "\n",
    "# # Normalize the data using z-score normalization\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "# # Scale the values to be within the range of 0 to 1\n",
    "# min_max_scaler = MinMaxScaler()\n",
    "# X = min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"tf.__operators__.add_176\" (type TFOpLambda).\n\nDimensions must be equal, but are 4096 and 32 for '{{node tf.__operators__.add_176/AddV2}} = AddV2[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes: [4096], [32].\n\nCall arguments received by layer \"tf.__operators__.add_176\" (type TFOpLambda):\n  • x=tf.Tensor(shape=(4096,), dtype=float32)\n  • y=tf.Tensor(shape=(32,), dtype=float32)\n  • name=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Alec\\Desktop\\Code\\HAR Research\\Virtual Data\\VAE-RNN.ipynb Cell 3\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Alec/Desktop/Code/HAR%20Research/Virtual%20Data/VAE-RNN.ipynb#W2sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m penalty \u001b[39m=\u001b[39m custom_penalty(outputs, normalized_min_max_values, input_dim)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Alec/Desktop/Code/HAR%20Research/Virtual%20Data/VAE-RNN.ipynb#W2sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m penalty \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m penalty_weight\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Alec/Desktop/Code/HAR%20Research/Virtual%20Data/VAE-RNN.ipynb#W2sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m vae_loss \u001b[39m=\u001b[39m K\u001b[39m.\u001b[39mmean(reconstruction_loss \u001b[39m+\u001b[39;49m kl_loss \u001b[39m+\u001b[39m penalty)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Alec/Desktop/Code/HAR%20Research/Virtual%20Data/VAE-RNN.ipynb#W2sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m vae\u001b[39m.\u001b[39madd_loss(vae_loss)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Alec/Desktop/Code/HAR%20Research/Virtual%20Data/VAE-RNN.ipynb#W2sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m vae\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\layers\\core\\tf_op_layer.py:119\u001b[0m, in \u001b[0;36mKerasOpDispatcher.handle\u001b[1;34m(self, op, args, kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39m\"\"\"Handle the specified operation with the specified arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[0;32m    116\u001b[0m     \u001b[39misinstance\u001b[39m(x, keras_tensor\u001b[39m.\u001b[39mKerasTensor)\n\u001b[0;32m    117\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten([args, kwargs])\n\u001b[0;32m    118\u001b[0m ):\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m TFOpLambda(op)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNOT_SUPPORTED\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"tf.__operators__.add_176\" (type TFOpLambda).\n\nDimensions must be equal, but are 4096 and 32 for '{{node tf.__operators__.add_176/AddV2}} = AddV2[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes: [4096], [32].\n\nCall arguments received by layer \"tf.__operators__.add_176\" (type TFOpLambda):\n  • x=tf.Tensor(shape=(4096,), dtype=float32)\n  • y=tf.Tensor(shape=(32,), dtype=float32)\n  • name=None"
     ]
    }
   ],
   "source": [
    "# Define the log directory for TensorBoard\n",
    "# log_dir = \"logs/\"\n",
    "\n",
    "# Create a callback for TensorBoard\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Prepare the data for input into the VAE model\n",
    "X = np.stack((timestamp, device_id, status, activity, activity_status), axis=1)\n",
    "\n",
    "# Normalize the data using minMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# print(X.head(20))\n",
    "# Find the corresponding normalized values\n",
    "temp_min_max_values = np.zeros((4, 5))\n",
    "temp_min_max_values[0, 0] = 0\n",
    "temp_min_max_values[1, 0] = processed_data['Timestamp'].nunique() - 1\n",
    "\n",
    "# M sensors\n",
    "temp_min_max_values[0, 1] = 3\n",
    "temp_min_max_values[1, 1] = 33\n",
    "temp_min_max_values[0, 2] = 54\n",
    "temp_min_max_values[1, 2] = 55  # 'OFF' and 'ON' status values for M sensors\n",
    "\n",
    "# T sensors\n",
    "temp_min_max_values[2, 1] = 34\n",
    "temp_min_max_values[3, 1] = 38\n",
    "temp_min_max_values[2, 2] = 0\n",
    "temp_min_max_values[3, 2] = 43  # Temperature values for T sensors\n",
    "\n",
    "# D sensors\n",
    "temp_min_max_values[0, 1] = 0\n",
    "temp_min_max_values[1, 1] = 2\n",
    "temp_min_max_values[0, 2] = 53\n",
    "temp_min_max_values[1, 2] = 56  # 'CLOSE' and 'OPEN' status values for D sensors\n",
    "\n",
    "temp_min_max_values[:, 4] = [0, 2, 0, 2]     # Activity Status values\n",
    "\n",
    "# Find the corresponding normalized values using the scaler\n",
    "normalized_min_max_values = scaler.transform(temp_min_max_values)\n",
    "\n",
    "# Use KMeans to cluster sequences into 14 different groups\n",
    "# kmeans = KMeans(n_clusters=14, random_state=0)\n",
    "# clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "timesteps = 128 # number of previous records considered\n",
    "input_dim = X.shape[1] # number of features, there are 5 features in the dataset\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, X, test_size=validation_split, shuffle=False)\n",
    "\n",
    "# Pad the data to ensure it is divisible by the desired shape\n",
    "remainder_train = X_train.shape[0] % (batch_size * timesteps)\n",
    "if remainder_train > 0:\n",
    "    X_train = np.concatenate([X_train[:-remainder_train], np.zeros((batch_size * timesteps - remainder_train, input_dim))])\n",
    "    y_train = np.concatenate([y_train[:-remainder_train], np.zeros((batch_size * timesteps - remainder_train, input_dim))])\n",
    "\n",
    "remainder_val = X_val.shape[0] % (batch_size * timesteps)\n",
    "if remainder_val > 0:\n",
    "    X_val = np.concatenate([X_val[:-remainder_val], np.zeros((batch_size * timesteps - remainder_val, input_dim))])\n",
    "    y_val = np.concatenate([y_val[:-remainder_val], np.zeros((batch_size * timesteps - remainder_val, input_dim))])\n",
    "\n",
    "# Reshape the datasets to have the correct shape for the model\n",
    "X_train = X_train.reshape((-1, timesteps, input_dim))\n",
    "y_train = y_train.reshape((-1, timesteps, input_dim))\n",
    "\n",
    "X_val = X_val.reshape((-1, timesteps, input_dim))\n",
    "y_val = y_val.reshape((-1, timesteps, input_dim))\n",
    "\n",
    "latent_dim = 2\n",
    "encoding_dim = 32\n",
    "\n",
    "# ==================== ENCODER ====================\n",
    "inputs = Input(batch_shape=(batch_size, timesteps, input_dim), name='encoder_input')\n",
    "x = LSTM(encoding_dim*2, return_sequences=True)(inputs)\n",
    "x = LSTM(encoding_dim, return_sequences=False)(x) \n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "# z_mean is the mean of the latent space\n",
    "# z_log_var is the variance of the latent space\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "# encoder.summary()\n",
    "\n",
    "# ================= LATENT SPACE ==================\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "# ==================== DECODER ====================\n",
    "x = Dense(timesteps * encoding_dim, activation='relu')(latent_inputs)\n",
    "x = Reshape((timesteps, encoding_dim))(x)\n",
    "x = LSTM(encoding_dim, return_sequences=True, input_shape=(timesteps, encoding_dim))(x)\n",
    "x = TimeDistributed(Dense(input_dim))(x)\n",
    "# LSTM layer in the decoder is used to reconstruct the original sequence\n",
    "\n",
    "# the VAE model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = Model(latent_inputs, x, name='decoder')\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "def custom_penalty(y_pred, normalized_min_max_values, input_dim):\n",
    "    # Device ID constraints\n",
    "    min_device_id_m = normalized_min_max_values[0, 1]\n",
    "    max_device_id_m = normalized_min_max_values[1, 1]\n",
    "    min_device_id_t = normalized_min_max_values[2, 1]\n",
    "    max_device_id_t = normalized_min_max_values[3, 1]\n",
    "    min_device_id_d = normalized_min_max_values[0, 1]\n",
    "    max_device_id_d = normalized_min_max_values[1, 1]\n",
    "\n",
    "    # Status constraints\n",
    "    min_status_m = normalized_min_max_values[0, 2]\n",
    "    max_status_m = normalized_min_max_values[1, 2]\n",
    "    min_status_t = normalized_min_max_values[2, 2]\n",
    "    max_status_t = normalized_min_max_values[3, 2]\n",
    "    min_status_d = normalized_min_max_values[0, 2]\n",
    "    max_status_d = normalized_min_max_values[1, 2]\n",
    "\n",
    "    # Reshape the flattened y_pred to have the correct shape\n",
    "    y_pred = K.reshape(y_pred, (-1, input_dim))\n",
    "\n",
    "    # Extract the 'Device ID' and 'Status' columns from the predicted values\n",
    "    y_pred_device_id = y_pred[:, 1]\n",
    "    y_pred_status = y_pred[:, 2]\n",
    "\n",
    "    # Calculate the penalty for values outside the desired range\n",
    "    m_device_penalty = K.sum(K.relu(y_pred_device_id - max_device_id_m) + K.relu(min_device_id_m - y_pred_device_id))\n",
    "    t_device_penalty = K.sum(K.relu(y_pred_device_id - max_device_id_t) + K.relu(min_device_id_t - y_pred_device_id))\n",
    "    d_device_penalty = K.sum(K.relu(y_pred_device_id - max_device_id_d) + K.relu(min_device_id_d - y_pred_device_id))\n",
    "\n",
    "    m_status_penalty = K.sum(K.relu(y_pred_status - max_status_m) + K.relu(min_status_m - y_pred_status))\n",
    "    t_status_penalty = K.sum(K.relu(y_pred_status - max_status_t) + K.relu(min_status_t - y_pred_status))\n",
    "    d_status_penalty = K.sum(K.relu(y_pred_status - max_status_d) + K.relu(min_status_d - y_pred_status))\n",
    "\n",
    "    penalty = m_device_penalty + t_device_penalty + d_device_penalty + m_status_penalty + t_status_penalty + d_status_penalty\n",
    "\n",
    "    return penalty\n",
    "\n",
    "# Loss function\n",
    "sample_size = K.prod(K.shape(inputs)[:-1])\n",
    "reconstruction_loss = binary_crossentropy(K.reshape(inputs, (sample_size, input_dim)), K.reshape(outputs, (sample_size, input_dim)))\n",
    "reconstruction_loss *= input_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "\n",
    "# Add the custom penalty to the loss function\n",
    "penalty_weight = 10.0  # Adjust the weight of the penalty term as needed\n",
    "# penalty = custom_penalty(y_pred=outputs)\n",
    "penalty = custom_penalty(outputs, normalized_min_max_values, input_dim)\n",
    "penalty *= penalty_weight\n",
    "\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss + penalty)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "# vae.summary()\n",
    "# callbacks=[tensorboard_callback]\n",
    "num_epochs = 300\n",
    "history = vae.fit(X_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
    "\n",
    "# plot_model(vae, to_file='model.png', show_shapes=True)\n",
    "# Use the encoder to generate embeddings for each sequence\n",
    "encoder_model = Model(inputs, z_mean)\n",
    "# print(encoder_model.layers[0].input_shape)\n",
    "\n",
    "X_embedded = encoder_model.predict(X_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n",
      "100/100 [==============================] - 1s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate a fake dataset using the VAE model\n",
    "n_samples = len(processed_data)\n",
    "\n",
    "# Sample from the latent space\n",
    "z_samples = np.random.normal(size=(n_samples, latent_dim))\n",
    "\n",
    "# Use the decoder to generate the output\n",
    "predicted_values = decoder.predict(z_samples)\n",
    "predicted_values = np.reshape(predicted_values, (n_samples, timesteps, input_dim))\n",
    "\n",
    "# Undo the normalization\n",
    "predicted_values = np.reshape(predicted_values, (-1, input_dim))\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Save the prediction data to a new file 'predicted_Data.csv'\n",
    "predicted_data = pd.DataFrame(predicted_values.reshape((-1, input_dim)), columns=['Timestamp', 'Device ID', 'Status', 'Activity', 'Activity Status'])\n",
    "\n",
    "with open('Predictions/Aruba_17_prediction.txt', 'w') as file:\n",
    "    for _, row in predicted_data.iterrows():\n",
    "        file.write(','.join(map(str, row.values)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss with x and y labels, and a grid\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "# Validation loss > training loss, underfitting\n",
    "# validation loss > training loss, overfitting, if it decreases and then increases again.\n",
    "# If they both decreease and stabilize at a specific point, it is an optimal fit.\n",
    "\n",
    "# Plot the evaluation loss vs the iterations\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the model\n",
    "# from keras.utils import plot_model\n",
    "\n",
    "# # Display the layers, number of layers, number of nodes etc\n",
    "# plot_model(vae, to_file='vae.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# # Load the image and display it\n",
    "# img = plt.imread('vae.png')\n",
    "# plt.figure(figsize=(16, 12))\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e3f0318fa44a63fbd15a81336d0e6b9929111f70e7cf4cecf151c11d26f00aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
