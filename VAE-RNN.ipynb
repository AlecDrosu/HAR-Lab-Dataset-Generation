{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, RepeatVector, TimeDistributed, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the original dataset\n",
    "processed_data = pd.read_csv('Processed Data/Aruba_17/processed_data.csv')\n",
    "# Find the maximum number that can be evenly divisible by 32, given the length of the dataset\n",
    "max_length = len(processed_data) - len(processed_data) % 32\n",
    "processed_data = processed_data.head(3200)\n",
    "\n",
    "# Extract the relevant columns from the dataset\n",
    "timestamp = processed_data['Timestamp'].values\n",
    "device_id = processed_data['Device ID'].values\n",
    "status = processed_data['Status'].values\n",
    "activity = processed_data['Activity'].values\n",
    "activity_status = processed_data['Activity Status'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack((timestamp, device_id, status, activity, activity_status), axis=1)\n",
    "\n",
    "# Normalize the data using z-score normalization\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Scale the values to be within the range of 0 to 1\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "# Export the normalized data to a csv file\n",
    "# np.savetxt('Processed Data/Aruba_17/normalized_data.csv', X, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "1/1 [==============================] - 9s 9s/step - loss: 3075.1372 - val_loss: 733.9366\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 2359.7056 - val_loss: 633.1518\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 1818.1725 - val_loss: 559.9404\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 1624.8889 - val_loss: 535.7073\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 1549.7104 - val_loss: 533.6772\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 1465.9426 - val_loss: 509.0505\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 1429.3860 - val_loss: 504.7993\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 1372.8521 - val_loss: 515.5835\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 1329.9780 - val_loss: 495.5468\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 1303.5734 - val_loss: 486.7992\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 1278.2524 - val_loss: 485.4204\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 1242.7573 - val_loss: 478.9293\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 1194.7576 - val_loss: 477.4849\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 1159.9529 - val_loss: 452.2721\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 1101.9944 - val_loss: 443.4045\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 1112.5959 - val_loss: 423.3210\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 1058.8705 - val_loss: 398.2973\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 988.5580 - val_loss: 341.4029\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 883.4182 - val_loss: 319.2081\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 748.2721 - val_loss: 245.2525\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 710.8250 - val_loss: 247.3237\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 657.1417 - val_loss: 247.5535\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 673.2601 - val_loss: 237.1407\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 559.3732 - val_loss: 262.6258\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 519.1952 - val_loss: 209.8404\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 562.8173 - val_loss: 184.9010\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 527.4207 - val_loss: 211.6722\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 468.2753 - val_loss: 198.0741\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 447.3528 - val_loss: 192.5878\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 451.6645 - val_loss: 193.1359\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 448.5404 - val_loss: 184.9214\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 397.7938 - val_loss: 244.4725\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 426.8643 - val_loss: 189.9661\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 412.6836 - val_loss: 196.3865\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 369.5325 - val_loss: 200.9323\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 361.0340 - val_loss: 215.0596\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 345.4645 - val_loss: 221.8346\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 358.8716 - val_loss: 233.9687\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 353.4405 - val_loss: 233.5722\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 362.1160 - val_loss: 223.4030\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 345.0933 - val_loss: 235.1116\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 353.6567 - val_loss: 191.7520\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 326.0922 - val_loss: 230.7588\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 351.1201 - val_loss: 226.4493\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 370.1437 - val_loss: 173.8027\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 328.3280 - val_loss: 192.6663\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 308.1519 - val_loss: 169.3367\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 291.5291 - val_loss: 169.2277\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 318.1454 - val_loss: 165.5181\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 307.6161 - val_loss: 167.7055\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 267.1938 - val_loss: 154.0695\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 266.1585 - val_loss: 148.9803\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 259.2118 - val_loss: 150.2180\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 263.0306 - val_loss: 159.8612\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 263.4820 - val_loss: 155.1975\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 281.1619 - val_loss: 144.2938\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 255.2839 - val_loss: 142.1393\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 265.6073 - val_loss: 139.2762\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 255.1934 - val_loss: 145.9412\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 239.1602 - val_loss: 141.7636\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 248.9809 - val_loss: 151.5148\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 241.3502 - val_loss: 143.0350\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 232.8900 - val_loss: 124.6887\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 240.9493 - val_loss: 139.8198\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 236.5646 - val_loss: 131.4777\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 231.6980 - val_loss: 126.7459\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 238.0330 - val_loss: 133.9597\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 221.8668 - val_loss: 127.6289\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 239.6476 - val_loss: 140.4182\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 222.8986 - val_loss: 127.5425\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 229.1501 - val_loss: 125.4662\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 222.1376 - val_loss: 116.3869\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 217.3349 - val_loss: 121.0259\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 228.0001 - val_loss: 119.0283\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 220.7625 - val_loss: 121.8008\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 218.4263 - val_loss: 115.6293\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 214.5446 - val_loss: 116.4972\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 214.6936 - val_loss: 115.6714\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 214.3132 - val_loss: 113.3144\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 213.2903 - val_loss: 109.7672\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 214.6668 - val_loss: 103.8608\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 207.2166 - val_loss: 108.7360\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 205.9362 - val_loss: 108.3057\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 206.1641 - val_loss: 103.2308\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 205.3214 - val_loss: 101.5735\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 207.5129 - val_loss: 104.5112\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 205.3944 - val_loss: 99.1672\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 205.3581 - val_loss: 102.6015\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 205.8541 - val_loss: 101.1388\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 200.1938 - val_loss: 98.7396\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 202.6360 - val_loss: 103.0105\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 202.8314 - val_loss: 99.6175\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 200.4220 - val_loss: 96.1201\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 198.9529 - val_loss: 97.2827\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 196.0669 - val_loss: 92.0229\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 198.0493 - val_loss: 91.0043\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 197.3255 - val_loss: 90.4420\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 196.2965 - val_loss: 89.6833\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 196.6153 - val_loss: 92.0148\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 197.4765 - val_loss: 86.0780\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 196.2216 - val_loss: 88.2318\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 194.0060 - val_loss: 85.4053\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 192.9565 - val_loss: 88.3603\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 200.0137 - val_loss: 87.2898\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 194.6660 - val_loss: 83.5504\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 195.8752 - val_loss: 83.7537\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 194.2272 - val_loss: 83.4553\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 191.4086 - val_loss: 83.1277\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 191.1864 - val_loss: 82.1124\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 192.2861 - val_loss: 81.3714\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 191.4139 - val_loss: 83.0345\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 191.1360 - val_loss: 78.8709\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 190.6629 - val_loss: 81.1392\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 189.3813 - val_loss: 78.9908\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 188.6127 - val_loss: 77.9583\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 187.7640 - val_loss: 79.0967\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 189.5736 - val_loss: 78.7070\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 188.2596 - val_loss: 77.7781\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 188.9880 - val_loss: 77.7532\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 186.6116 - val_loss: 77.7053\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 186.8152 - val_loss: 78.2775\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 186.8210 - val_loss: 77.1277\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 186.7837 - val_loss: 74.9594\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 186.3548 - val_loss: 73.9894\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 185.8243 - val_loss: 75.5215\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 186.1064 - val_loss: 74.7107\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 185.8815 - val_loss: 74.2301\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 185.8972 - val_loss: 72.7244\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 183.6296 - val_loss: 73.4108\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 183.7159 - val_loss: 72.3685\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 183.4287 - val_loss: 72.0449\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 183.4919 - val_loss: 72.9827\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 182.7212 - val_loss: 71.3627\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 183.1624 - val_loss: 70.5580\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 182.0325 - val_loss: 71.7292\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 181.4637 - val_loss: 69.7780\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 182.9637 - val_loss: 70.7142\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 181.7727 - val_loss: 70.3984\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 181.0089 - val_loss: 70.3295\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 183.0674 - val_loss: 70.6503\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 183.6145 - val_loss: 68.1596\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 183.1976 - val_loss: 68.3708\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 180.8239 - val_loss: 70.6133\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 180.6028 - val_loss: 68.2403\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 180.5494 - val_loss: 68.6845\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 180.4340 - val_loss: 67.4554\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 179.3599 - val_loss: 65.9744\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 179.8536 - val_loss: 66.2009\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 178.7030 - val_loss: 66.0568\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 179.9558 - val_loss: 65.6413\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 180.7692 - val_loss: 66.3872\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 178.5733 - val_loss: 68.5946\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 178.8194 - val_loss: 65.8425\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 180.3762 - val_loss: 66.1575\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 178.9728 - val_loss: 66.0564\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 178.5226 - val_loss: 64.4982\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 178.6677 - val_loss: 65.6007\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 177.9908 - val_loss: 64.8820\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 178.8492 - val_loss: 64.1385\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 176.4677 - val_loss: 63.7707\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 177.8118 - val_loss: 64.3633\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 176.7849 - val_loss: 64.6117\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 176.6533 - val_loss: 62.0297\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 176.6583 - val_loss: 61.6868\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 176.6644 - val_loss: 63.3197\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 177.0548 - val_loss: 61.9053\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 176.4631 - val_loss: 62.0742\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 176.9818 - val_loss: 61.0274\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 176.5333 - val_loss: 61.5006\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 176.1206 - val_loss: 61.5945\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 175.2070 - val_loss: 60.6764\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 174.7665 - val_loss: 60.7330\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 176.0272 - val_loss: 60.1407\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 174.5493 - val_loss: 59.9076\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 175.7476 - val_loss: 59.7160\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 175.2918 - val_loss: 59.3810\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 174.5443 - val_loss: 59.2088\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 173.2540 - val_loss: 58.2300\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 175.5424 - val_loss: 58.1121\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 175.6638 - val_loss: 59.4597\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 175.0012 - val_loss: 58.3403\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 172.7919 - val_loss: 57.9277\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 173.1812 - val_loss: 57.3598\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 173.0423 - val_loss: 57.3321\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 173.2490 - val_loss: 56.7490\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 173.4581 - val_loss: 57.5983\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 173.2605 - val_loss: 56.4958\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 172.3692 - val_loss: 56.4052\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 174.2729 - val_loss: 56.0980\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 173.0052 - val_loss: 55.6139\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 172.4538 - val_loss: 55.6348\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 173.0590 - val_loss: 55.3150\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 171.9273 - val_loss: 55.7016\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 172.6054 - val_loss: 54.5276\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 171.4548 - val_loss: 55.2414\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 171.9253 - val_loss: 55.2483\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 171.3517 - val_loss: 54.2898\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 171.6795 - val_loss: 53.9900\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 171.6056 - val_loss: 54.9833\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 170.5788 - val_loss: 52.9085\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 170.2282 - val_loss: 53.7132\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 170.2175 - val_loss: 52.8566\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 170.4443 - val_loss: 52.3114\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 170.2234 - val_loss: 52.4909\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 170.2371 - val_loss: 52.5390\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 169.0282 - val_loss: 51.9167\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 169.8909 - val_loss: 51.7500\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 170.1188 - val_loss: 51.8206\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 170.1831 - val_loss: 51.9270\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 170.5467 - val_loss: 51.3525\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 170.1573 - val_loss: 52.1536\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 168.8329 - val_loss: 51.1305\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 168.7590 - val_loss: 50.6937\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 168.7723 - val_loss: 50.8766\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 168.8234 - val_loss: 50.3532\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 168.8813 - val_loss: 49.5580\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 169.7769 - val_loss: 49.7868\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 168.2614 - val_loss: 50.2911\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 168.5591 - val_loss: 49.2190\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 168.8510 - val_loss: 49.7107\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 167.1668 - val_loss: 49.1773\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 167.5287 - val_loss: 48.7387\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 168.2346 - val_loss: 49.0059\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 167.6077 - val_loss: 48.4157\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 167.3194 - val_loss: 48.0485\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 166.5937 - val_loss: 49.8130\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 167.6140 - val_loss: 47.5914\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 167.0728 - val_loss: 47.6183\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 167.7186 - val_loss: 48.1433\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 166.3357 - val_loss: 47.7570\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 166.9051 - val_loss: 47.9665\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 166.6136 - val_loss: 48.2057\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 166.5567 - val_loss: 47.3521\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 166.4140 - val_loss: 47.2241\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 166.0984 - val_loss: 47.0452\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 166.2522 - val_loss: 46.9414\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 165.9426 - val_loss: 47.2680\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 165.5669 - val_loss: 46.6131\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 168.7877 - val_loss: 46.8427\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 165.3166 - val_loss: 47.8379\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 166.2478 - val_loss: 46.8363\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 166.3402 - val_loss: 45.7761\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 166.7474 - val_loss: 46.4993\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 167.5042 - val_loss: 45.5593\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 166.3834 - val_loss: 48.6200\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 165.7795 - val_loss: 46.1046\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 165.9934 - val_loss: 45.8542\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 165.3057 - val_loss: 45.5095\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 165.6102 - val_loss: 45.7129\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 165.0099 - val_loss: 45.6492\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 165.8801 - val_loss: 45.2059\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 164.9255 - val_loss: 45.6520\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 165.2944 - val_loss: 45.1588\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 165.6859 - val_loss: 45.8139\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 165.6144 - val_loss: 45.2816\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 165.1085 - val_loss: 45.2955\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 165.2854 - val_loss: 45.0870\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 164.8170 - val_loss: 44.6079\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 164.5106 - val_loss: 45.1317\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 164.4445 - val_loss: 45.7572\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 164.5667 - val_loss: 45.0022\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 164.7094 - val_loss: 44.6632\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 164.8902 - val_loss: 45.8684\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 164.5557 - val_loss: 45.1505\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 164.4079 - val_loss: 45.0648\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 165.0110 - val_loss: 44.7706\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 164.0436 - val_loss: 44.7449\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 164.8067 - val_loss: 44.3545\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 164.6089 - val_loss: 44.5489\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 164.5256 - val_loss: 45.6841\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 163.8601 - val_loss: 44.7822\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 164.3875 - val_loss: 44.2106\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 164.3059 - val_loss: 44.6615\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 163.7154 - val_loss: 45.2255\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 164.1849 - val_loss: 43.7599\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 164.3138 - val_loss: 43.9274\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 163.7156 - val_loss: 43.8849\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 163.5736 - val_loss: 44.2982\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 163.8111 - val_loss: 43.7301\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 163.9564 - val_loss: 43.4224\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 163.3604 - val_loss: 43.4610\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 164.4886 - val_loss: 43.6850\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 163.4840 - val_loss: 43.4785\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 163.7935 - val_loss: 43.8938\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 163.5698 - val_loss: 43.4608\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 163.5393 - val_loss: 43.2543\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 163.3018 - val_loss: 43.8799\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 162.9932 - val_loss: 42.9719\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 163.4180 - val_loss: 43.7130\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 162.6925 - val_loss: 43.2924\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 163.1724 - val_loss: 43.7168\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 163.1516 - val_loss: 43.6632\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 162.8341 - val_loss: 43.5474\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 163.4507 - val_loss: 42.4260\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 163.4705 - val_loss: 44.4267\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 163.0714 - val_loss: 42.3818\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 162.9710 - val_loss: 42.5753\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 162.3460 - val_loss: 43.0582\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 163.3117 - val_loss: 43.1361\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 162.8705 - val_loss: 42.1280\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 162.3418 - val_loss: 42.7749\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 162.5741 - val_loss: 42.6182\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 162.8724 - val_loss: 42.3179\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 162.3427 - val_loss: 42.7190\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 162.6530 - val_loss: 42.4852\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 162.6854 - val_loss: 42.3891\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 161.9318 - val_loss: 43.0108\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 162.3434 - val_loss: 41.9079\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 162.7975 - val_loss: 43.0056\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 162.7983 - val_loss: 41.6703\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 162.3589 - val_loss: 42.6492\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 162.4234 - val_loss: 43.0957\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 162.0591 - val_loss: 42.0076\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 162.4228 - val_loss: 41.5838\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 162.2712 - val_loss: 42.3481\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 164.4730 - val_loss: 42.3289\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 162.0413 - val_loss: 42.2640\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 162.3058 - val_loss: 41.9786\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 161.5831 - val_loss: 42.8872\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 162.4382 - val_loss: 41.7168\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 162.0941 - val_loss: 42.0276\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 161.7807 - val_loss: 41.4820\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 161.3748 - val_loss: 42.9894\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 162.7172 - val_loss: 41.8539\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 161.3864 - val_loss: 41.3331\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 162.3447 - val_loss: 42.0364\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 163.2087 - val_loss: 41.5853\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 162.8230 - val_loss: 43.3191\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 161.2578 - val_loss: 41.6687\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 162.1364 - val_loss: 41.3274\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 161.3129 - val_loss: 42.0313\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 161.3062 - val_loss: 42.2185\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 162.6666 - val_loss: 41.2890\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 162.0556 - val_loss: 42.3165\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 161.7349 - val_loss: 41.3129\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 161.5776 - val_loss: 41.9411\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 161.9980 - val_loss: 41.0448\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 161.7917 - val_loss: 41.5588\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 161.9072 - val_loss: 40.9257\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 161.8339 - val_loss: 41.1266\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 163.2915 - val_loss: 41.3486\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 161.6478 - val_loss: 41.5211\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 161.9005 - val_loss: 41.9278\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 161.6052 - val_loss: 41.0731\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 161.3750 - val_loss: 41.3338\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 161.4244 - val_loss: 41.4319\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 161.3111 - val_loss: 41.4113\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 161.5184 - val_loss: 41.9280\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 161.7162 - val_loss: 41.0491\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 161.6749 - val_loss: 41.6493\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 161.8006 - val_loss: 41.5522\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 161.0090 - val_loss: 41.2042\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 161.3268 - val_loss: 41.3317\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 161.5993 - val_loss: 41.2517\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 161.5908 - val_loss: 41.3703\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 161.2112 - val_loss: 41.1091\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 161.4743 - val_loss: 41.7316\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 161.0372 - val_loss: 40.9574\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 161.4567 - val_loss: 41.2196\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 161.0531 - val_loss: 41.1229\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 161.3026 - val_loss: 40.7248\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 161.1292 - val_loss: 41.1161\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 162.1396 - val_loss: 41.0896\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 161.1914 - val_loss: 41.5928\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 160.9350 - val_loss: 41.0164\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 160.8921 - val_loss: 41.1673\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 161.0282 - val_loss: 40.6092\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 160.9717 - val_loss: 40.8908\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 161.1461 - val_loss: 40.7752\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 160.8545 - val_loss: 40.7376\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 161.4549 - val_loss: 41.1515\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 160.8587 - val_loss: 40.5095\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 160.8216 - val_loss: 40.5989\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 160.6807 - val_loss: 40.4255\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 161.4017 - val_loss: 40.9042\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 161.2748 - val_loss: 40.7741\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 160.9299 - val_loss: 40.3846\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 161.2199 - val_loss: 40.9736\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 160.9850 - val_loss: 40.7688\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 161.2533 - val_loss: 40.5454\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 161.3417 - val_loss: 40.8786\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 160.4552 - val_loss: 40.5128\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 160.7485 - val_loss: 41.0899\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 162.3423 - val_loss: 40.9450\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 160.7643 - val_loss: 40.6947\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 160.3926 - val_loss: 40.7827\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 161.6206 - val_loss: 40.9543\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 160.9996 - val_loss: 40.7797\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 160.2096 - val_loss: 40.3655\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 160.8048 - val_loss: 40.4189\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 160.1280 - val_loss: 40.3964\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 160.5836 - val_loss: 40.7035\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 161.2234 - val_loss: 40.3316\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 160.3687 - val_loss: 42.0586\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 160.5438 - val_loss: 40.4932\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 160.8188 - val_loss: 40.4450\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 160.7800 - val_loss: 40.9073\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 160.3219 - val_loss: 40.1962\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 160.7087 - val_loss: 40.3245\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 160.2433 - val_loss: 40.8099\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 160.3223 - val_loss: 40.3578\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 160.6855 - val_loss: 41.0901\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 160.2598 - val_loss: 40.8832\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 160.1576 - val_loss: 40.9924\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 160.2008 - val_loss: 40.1558\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 160.2494 - val_loss: 41.0560\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 161.4617 - val_loss: 40.6315\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 160.6116 - val_loss: 40.3990\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 160.0489 - val_loss: 40.1901\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 160.0775 - val_loss: 40.7078\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 159.9510 - val_loss: 40.4106\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 161.0137 - val_loss: 39.9865\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 160.2133 - val_loss: 41.4994\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 160.0444 - val_loss: 40.1692\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 160.4705 - val_loss: 40.3469\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 160.8540 - val_loss: 40.1436\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 160.3367 - val_loss: 40.6532\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 160.4586 - val_loss: 40.0460\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 160.4522 - val_loss: 41.3315\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 160.1782 - val_loss: 40.8453\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 160.3416 - val_loss: 40.2065\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 160.1746 - val_loss: 40.2299\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 160.6089 - val_loss: 40.2348\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 160.4905 - val_loss: 40.3083\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 159.6765 - val_loss: 40.3488\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 160.0734 - val_loss: 40.3116\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 160.1511 - val_loss: 40.0593\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 160.2822 - val_loss: 40.6592\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 159.9419 - val_loss: 40.6400\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 160.4655 - val_loss: 40.2260\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 160.4485 - val_loss: 40.7997\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 160.1120 - val_loss: 40.6016\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 159.9064 - val_loss: 40.7756\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 159.5518 - val_loss: 40.3285\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 159.9813 - val_loss: 40.8756\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 159.9215 - val_loss: 39.9694\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 159.8028 - val_loss: 40.2793\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 160.2697 - val_loss: 40.0875\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 159.7111 - val_loss: 40.2758\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 159.8130 - val_loss: 40.3351\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 159.8597 - val_loss: 40.1511\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 159.6246 - val_loss: 40.7214\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 160.2948 - val_loss: 39.9164\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 160.5603 - val_loss: 40.2267\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 159.9352 - val_loss: 40.4077\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 160.1852 - val_loss: 40.7651\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 159.6705 - val_loss: 40.3328\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 159.7102 - val_loss: 40.2437\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 160.7372 - val_loss: 39.9496\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 159.8109 - val_loss: 40.4323\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 160.0813 - val_loss: 40.4036\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 160.5083 - val_loss: 40.2408\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 159.9386 - val_loss: 40.0279\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 160.3668 - val_loss: 40.2815\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 159.8813 - val_loss: 39.9454\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 159.2969 - val_loss: 40.1720\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 159.6895 - val_loss: 40.3552\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 160.1915 - val_loss: 40.1544\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 159.5915 - val_loss: 40.0568\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 159.8719 - val_loss: 40.2067\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 159.9262 - val_loss: 39.9620\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 160.5807 - val_loss: 40.0228\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 159.4718 - val_loss: 39.7364\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 160.6715 - val_loss: 41.1681\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 159.5839 - val_loss: 39.8738\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 159.2267 - val_loss: 39.5648\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 160.0049 - val_loss: 39.5527\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 159.5317 - val_loss: 40.2534\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 159.5607 - val_loss: 40.4524\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 158.9213 - val_loss: 39.8801\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 160.3016 - val_loss: 39.7689\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 159.3304 - val_loss: 40.1574\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 160.0155 - val_loss: 40.4305\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 160.6928 - val_loss: 39.8311\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 160.3398 - val_loss: 40.1362\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 160.4194 - val_loss: 40.0840\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 159.4987 - val_loss: 40.5165\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 159.7483 - val_loss: 40.3579\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 159.9743 - val_loss: 39.7614\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 159.7326 - val_loss: 40.2784\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 160.0563 - val_loss: 39.9784\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 159.6358 - val_loss: 39.7562\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 159.7635 - val_loss: 40.4916\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 159.9336 - val_loss: 40.5164\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 159.4900 - val_loss: 39.9517\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 159.3585 - val_loss: 40.6231\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 159.5147 - val_loss: 40.3999\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 159.3213 - val_loss: 39.9828\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 159.5605 - val_loss: 39.8688\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 159.4512 - val_loss: 39.7591\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 159.5369 - val_loss: 39.6690\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 159.6181 - val_loss: 39.7036\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 160.3399 - val_loss: 40.1598\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 159.5546 - val_loss: 40.1217\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 160.2172 - val_loss: 39.8133\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 159.5765 - val_loss: 39.9991\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 159.2620 - val_loss: 39.9276\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 159.2659 - val_loss: 39.8277\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 159.2654 - val_loss: 39.3562\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 159.4418 - val_loss: 39.6554\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 159.7958 - val_loss: 39.9984\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 159.4988 - val_loss: 39.6231\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 159.8478 - val_loss: 39.6479\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 159.4164 - val_loss: 40.8353\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 159.3401 - val_loss: 39.9785\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 160.0174 - val_loss: 40.1282\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 159.3194 - val_loss: 39.9685\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 158.8788 - val_loss: 40.5978\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 159.1018 - val_loss: 39.8613\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 159.6594 - val_loss: 39.6191\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 159.0362 - val_loss: 39.8080\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 159.3275 - val_loss: 40.2467\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 159.5758 - val_loss: 39.7459\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 159.5256 - val_loss: 40.1694\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 159.3751 - val_loss: 40.3507\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 158.9542 - val_loss: 39.9827\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 159.0719 - val_loss: 39.6059\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 159.8507 - val_loss: 39.8349\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 159.4696 - val_loss: 40.0654\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 159.3058 - val_loss: 39.3518\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 158.8418 - val_loss: 39.9522\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 158.9877 - val_loss: 39.7452\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 158.9572 - val_loss: 39.4160\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 158.6700 - val_loss: 39.7755\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 158.6992 - val_loss: 39.4346\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 158.8907 - val_loss: 39.5273\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 159.3223 - val_loss: 39.2108\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 158.9984 - val_loss: 39.6856\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 158.8634 - val_loss: 39.3479\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 160.2469 - val_loss: 39.3747\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 159.0632 - val_loss: 39.8656\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 159.8268 - val_loss: 39.4210\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 158.8632 - val_loss: 41.3443\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 159.4650 - val_loss: 39.0468\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 158.9011 - val_loss: 40.5627\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 158.8990 - val_loss: 39.0256\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 158.8769 - val_loss: 39.7260\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 158.9363 - val_loss: 39.5846\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 159.1280 - val_loss: 39.4795\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 159.1444 - val_loss: 39.6642\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 158.8029 - val_loss: 40.5755\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 159.1398 - val_loss: 39.5018\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 159.3095 - val_loss: 39.7752\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 159.2845 - val_loss: 40.2497\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 159.2295 - val_loss: 39.5679\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 158.6852 - val_loss: 39.5855\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 158.8490 - val_loss: 39.5480\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 159.3907 - val_loss: 39.5761\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 159.1992 - val_loss: 39.2071\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 158.7842 - val_loss: 39.5403\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 159.0362 - val_loss: 39.6968\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 158.4302 - val_loss: 39.3792\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 158.9905 - val_loss: 39.2906\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 159.1195 - val_loss: 39.5197\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 158.7773 - val_loss: 39.2270\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 158.6430 - val_loss: 39.4106\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 158.7314 - val_loss: 39.4706\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 158.5985 - val_loss: 39.8900\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 158.8088 - val_loss: 39.3583\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 159.0228 - val_loss: 39.6552\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 161.4545 - val_loss: 39.3895\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 158.7536 - val_loss: 39.4624\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 158.9473 - val_loss: 39.3630\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 159.4191 - val_loss: 39.4594\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 158.9582 - val_loss: 39.8953\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 159.0509 - val_loss: 39.7140\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 159.0950 - val_loss: 39.7189\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 158.7325 - val_loss: 39.4497\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 158.1932 - val_loss: 39.5169\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 158.6574 - val_loss: 39.1023\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 159.5248 - val_loss: 40.4005\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 158.6624 - val_loss: 39.8170\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 158.8279 - val_loss: 39.1966\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 158.8386 - val_loss: 39.8841\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 158.7103 - val_loss: 39.6205\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 158.5039 - val_loss: 40.0815\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 159.3160 - val_loss: 39.2987\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 158.7415 - val_loss: 39.3401\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 158.5425 - val_loss: 39.4326\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 159.0007 - val_loss: 39.3577\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 158.7159 - val_loss: 39.2955\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 160.0588 - val_loss: 39.2532\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 160.9976 - val_loss: 39.1990\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 159.0477 - val_loss: 39.3691\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 159.2043 - val_loss: 39.5493\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 158.8316 - val_loss: 39.5490\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 159.1500 - val_loss: 39.5559\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 159.2346 - val_loss: 39.8455\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 159.2650 - val_loss: 39.4847\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 159.7513 - val_loss: 40.0149\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 159.5047 - val_loss: 39.5655\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 158.9891 - val_loss: 39.5561\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 159.1134 - val_loss: 39.5051\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 159.6532 - val_loss: 39.7746\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 159.6402 - val_loss: 39.4763\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 159.6249 - val_loss: 39.4459\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 158.8016 - val_loss: 40.1193\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 158.9346 - val_loss: 39.7372\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 158.5919 - val_loss: 39.7099\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 159.0418 - val_loss: 39.7477\n",
      "1/1 [==============================] - 1s 851ms/step\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for input into the VAE model\n",
    "# X = np.stack((timestamp, device_id, status, activity, activity_status), axis=1)\n",
    "\n",
    "# # Normalize the data using minMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "# print(X.head(20))\n",
    "\n",
    "# Use KMeans to cluster sequences into 14 different groups\n",
    "kmeans = KMeans(n_clusters=14, random_state=0)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "timesteps = 128 # number of previous records considered\n",
    "input_dim = X.shape[1] # number of features, there are 5 features in the dataset\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, clusters, test_size=validation_split, shuffle=False)\n",
    "\n",
    "# Pad the data to ensure it is divisible by the desired shape\n",
    "remainder_train = X_train.shape[0] % (batch_size * timesteps)\n",
    "if remainder_train > 0:\n",
    "    X_train = np.concatenate([X_train, np.zeros((batch_size * timesteps - remainder_train, input_dim))])\n",
    "    y_train = np.concatenate([y_train, np.zeros((batch_size * timesteps - remainder_train,))])\n",
    "    \n",
    "remainder_val = X_val.shape[0] % (batch_size * timesteps)\n",
    "if remainder_val > 0:\n",
    "    X_val = np.concatenate([X_val, np.zeros((batch_size * timesteps - remainder_val, input_dim))])\n",
    "    y_val = np.concatenate([y_val, np.zeros((batch_size * timesteps - remainder_val,))])\n",
    "\n",
    "# Reshape the datasets to have the correct shape for the model\n",
    "X_train = X_train.reshape((-1, timesteps, input_dim))\n",
    "y_train = y_train.reshape((-1, timesteps))\n",
    "\n",
    "X_val = X_val.reshape((-1, timesteps, input_dim))\n",
    "y_val = y_val.reshape((-1, timesteps))\n",
    "\n",
    "latent_dim = 2\n",
    "encoding_dim = 32\n",
    "\n",
    "# Set the input shape for the autoencoder model\n",
    "inputs = Input(batch_shape=(batch_size, timesteps, input_dim), name='encoder_input')\n",
    "x = LSTM(encoding_dim*2, return_sequences=True)(inputs) # Add LSTM layer with return_sequences set to True\n",
    "x = LSTM(encoding_dim, return_sequences=False)(x) # Add another LSTM layer with return_sequences set to False\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "# encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "# encoder.summary()\n",
    "\n",
    "# The decoder\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(timesteps * encoding_dim, activation='relu')(latent_inputs)\n",
    "x = Reshape((timesteps, encoding_dim))(x)\n",
    "x = LSTM(encoding_dim, return_sequences=True, input_shape=(timesteps, encoding_dim))(x)\n",
    "x = TimeDistributed(Dense(input_dim))(x)\n",
    "# x = Dense(encoding_dim, activation='relu')(latent_inputs)\n",
    "# x = RepeatVector(timesteps)(x)\n",
    "# x = LSTM(encoding_dim, return_sequences=True)(x)\n",
    "# x = LSTM(encoding_dim*2, return_sequences=True)(x)\n",
    "# outputs = TimeDistributed(Dense(input_dim))(x)\n",
    "# outputs = Flatten()(outputs)\n",
    "# decoder_outputs = Dense(input_dim, activation='sigmoid')(x)\n",
    "\n",
    "# decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "# decoder.summary()\n",
    "\n",
    "# vae_outputs = decoder(encoder(inputs)[2])\n",
    "# vae_outputs = decoder(Lambda(sampling)([z_mean, z_log_var]))\n",
    "# vae = Model(inputs, vae_outputs, name='vae')\n",
    "\n",
    "# Define the VAE model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = Model(latent_inputs, x, name='decoder')\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n",
    "# reconstruction_loss = K.mean(reconstruction_loss)\n",
    "reconstruction_loss *= timesteps * input_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "# kl_loss = K.mean(kl_loss, axis=-1)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "# vae_loss = reconstruction_loss + kl_loss\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "# vae.summary()\n",
    "\n",
    "num_epochs = 600\n",
    "history = vae.fit(X_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
    "\n",
    "# Use the encoder to generate embeddings for each sequence\n",
    "encoder_model = Model(inputs, z_mean)\n",
    "# print(encoder_model.layers[0].input_shape)\n",
    "\n",
    "X_embedded = encoder_model.predict(X_train, batch_size=batch_size)\n",
    "\n",
    "# Train a classifier on the embeddings\n",
    "classifier = KMeans(n_clusters=14, random_state=0)\n",
    "y_pred = classifier.fit_predict(X_embedded)\n",
    "\n",
    "# Generate a fake dataset using the VAE model\n",
    "# n_samples = len(processed_data)\n",
    "# print(n_samples)\n",
    "# noise = np.random.normal(size=(n_samples, 5 - latent_dim))\n",
    "# noise = np.concatenate([noise, np.zeros((n_samples, latent_dim))], axis=-1)\n",
    "# # reshape noise to have the correct shape\n",
    "# noise = noise.reshape((int(noise.shape[0]/timesteps), timesteps, input_dim))\n",
    "# predicted_values = vae.predict(noise, batch_size=batch_size)\n",
    "\n",
    "# predicted_values = predicted_values.reshape((predicted_values.shape[0] * predicted_values.shape[1], predicted_values.shape[2]))\n",
    "\n",
    "# # undo the normalization\n",
    "# predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# # Round each of the values in the array to the nearest integer\n",
    "# predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# # Assign cluster labels to each of the predicted values\n",
    "# y_pred = kmeans.predict(predicted_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Older model code"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate a fake dataset using the VAE model\n",
    "n_samples = len(X_train)\n",
    "print(processed_data.shape)\n",
    "noise = np.random.normal(size=(n_samples, timesteps, input_dim))\n",
    "predicted_values = vae.predict(noise, batch_size=batch_size)\n",
    "# reshape predicted values to have the correct shape\n",
    "predicted_values = np.reshape(predicted_values, (n_samples*timesteps, input_dim))\n",
    "\n",
    "# undo the normalization\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Assign cluster labels to each of the predicted values\n",
    "y_pred = classifier.predict(encoder_model.predict(predicted_values, batch_size=batch_size))\n",
    "\n",
    "# Reshape y_pred to have the same shape as predicted_values\n",
    "y_pred = np.reshape(y_pred, (n_samples, timesteps))\n",
    "\n",
    "# Flatten the y_pred array to a 1D array of cluster labels\n",
    "y_pred = y_pred.flatten()\n",
    "\n",
    "# Create a new DataFrame with the desired column names and values\n",
    "predicted_data = pd.DataFrame.from_records(predicted_values, columns=['Timestamp', 'Device ID', 'Status', 'Activity', 'Activity Status'])\n",
    "predicted_data['Cluster'] = y_pred\n",
    "predicted_data = predicted_data.groupby('Timestamp', group_keys=False).apply(lambda x: x.sample(timesteps)).reset_index(drop=True)\n",
    "predicted_data.to_csv('Predictions/Aruba_17_prediction.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 5s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate a fake dataset using the VAE model\n",
    "n_samples = len(processed_data)\n",
    "\n",
    "noise = np.random.normal(size=(n_samples, timesteps, input_dim))\n",
    "predicted_values = vae.predict(noise, batch_size=batch_size)\n",
    "# reshape predicted values to have the correct shape\n",
    "predicted_values = np.reshape(predicted_values, (n_samples, timesteps, input_dim))\n",
    "\n",
    "# undo the normalization\n",
    "predicted_values = np.reshape(predicted_values, (-1, input_dim))\n",
    "predicted_values = min_max_scaler.inverse_transform(predicted_values)\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Reshape predicted_values to match the input shape of encoder_model\n",
    "# predicted_values = np.reshape(predicted_values, (n_samples, timesteps, input_dim))\n",
    "\n",
    "# # Assign cluster labels to each of the predicted values\n",
    "# y_pred = classifier.predict(encoder_model.predict(predicted_values, batch_size=batch_size))\n",
    "# # Print all information of the y_pred line above\n",
    "\n",
    "# # Reshape y_pred to match the shape of predicted_values\n",
    "# y_pred = np.reshape(y_pred, (n_samples, timesteps))\n",
    "\n",
    "# Save the prediction data to a new file 'predicted_Data.csv'\n",
    "predicted_data = pd.DataFrame(predicted_values.reshape((-1, input_dim)), columns=['Timestamp', 'Device ID', 'Status', 'Activity', 'Activity Status'])\n",
    "# predicted_data['Cluster'] = y_pred.reshape(-1)\n",
    "# predicted_data.to_csv('Predictions/Aruba_17_prediction.csv', index=False)\n",
    "with open('Predictions/Aruba_17_prediction.txt', 'w') as file:\n",
    "    for _, row in predicted_data.iterrows():\n",
    "        file.write(','.join(map(str, row.values)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss with x and y labels, and a grid\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "# Validation loss > training loss, underfitting\n",
    "# validation loss > training loss, overfitting, if it decreases and then increases again.\n",
    "# If they both decreease and stabilize at a specific point, it is an optimal fit.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e3f0318fa44a63fbd15a81336d0e6b9929111f70e7cf4cecf151c11d26f00aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
