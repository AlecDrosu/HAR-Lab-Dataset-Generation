{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, RepeatVector, TimeDistributed, Flatten\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the original dataset\n",
    "processed_data = pd.read_csv('Processed Data/Aruba_17/processed_data.csv')\n",
    "# only use the first 1000 rows\n",
    "processed_data = processed_data.head(1280000)\n",
    "# Extract the relevant columns from the dataset\n",
    "timestamp = processed_data['Timestamp'].values\n",
    "device_id = processed_data['Device ID'].values\n",
    "status = processed_data['Status'].values\n",
    "activity = processed_data['Activity'].values\n",
    "activity_status = processed_data['Activity Status'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 8s 431ms/step - loss: 6.0750 - val_loss: 3.2173\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 4.0794 - val_loss: 2.5147\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 2.8723 - val_loss: 1.8188\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 2.0346 - val_loss: 1.2301\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 1.4442 - val_loss: 1.1768\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.9939 - val_loss: 1.5135\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.8540 - val_loss: 1.5590\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.7766 - val_loss: 1.9613\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.6852 - val_loss: 2.0480\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.6482 - val_loss: 2.1770\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.6283 - val_loss: 2.1427\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.6005 - val_loss: 2.1604\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5813 - val_loss: 2.2756\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5630 - val_loss: 2.2279\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.5641 - val_loss: 2.3235\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.5499 - val_loss: 2.4404\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5439 - val_loss: 2.3851\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.5402 - val_loss: 2.3313\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.5333 - val_loss: 2.4152\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.5292 - val_loss: 2.3539\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5228 - val_loss: 2.3731\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5119 - val_loss: 2.4297\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5106 - val_loss: 2.4616\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.5044 - val_loss: 2.4197\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4927 - val_loss: 2.4604\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4992 - val_loss: 2.5401\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4959 - val_loss: 2.4609\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4916 - val_loss: 2.5096\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4909 - val_loss: 2.5551\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4888 - val_loss: 2.4847\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.4806 - val_loss: 2.5344\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4783 - val_loss: 2.5651\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4773 - val_loss: 2.4904\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4720 - val_loss: 2.5760\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4744 - val_loss: 2.5528\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4696 - val_loss: 2.5252\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4608 - val_loss: 2.5847\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4618 - val_loss: 2.5916\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4653 - val_loss: 2.5638\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4548 - val_loss: 2.5769\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4589 - val_loss: 2.5581\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4524 - val_loss: 2.5702\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4528 - val_loss: 2.5753\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4549 - val_loss: 2.5628\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4482 - val_loss: 2.5670\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4439 - val_loss: 2.5529\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4474 - val_loss: 2.6227\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4480 - val_loss: 2.5856\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.4454 - val_loss: 2.5757\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4414 - val_loss: 2.5868\n",
      "The shape of the encoder model is:  [(32, 32, 5)]\n",
      "5/5 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for input into the VAE model\n",
    "X = np.stack((timestamp, device_id, status, activity, activity_status), axis=1)\n",
    "\n",
    "# Normalize the data using minMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Use KMeans to cluster sequences into 14 different groups\n",
    "kmeans = KMeans(n_clusters=14, random_state=0)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "timesteps = 32 # number of previous records considered\n",
    "input_dim = X.shape[1] # number of features, there are 5 features in the dataset\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, clusters, test_size=validation_split, shuffle=False)\n",
    "\n",
    "# Pad the data to ensure it is divisible by the desired shape\n",
    "remainder_train = X_train.shape[0] % (batch_size * timesteps)\n",
    "if remainder_train > 0:\n",
    "    X_train = np.concatenate([X_train, np.zeros((batch_size * timesteps - remainder_train, input_dim))])\n",
    "    y_train = np.concatenate([y_train, np.zeros((batch_size * timesteps - remainder_train,))])\n",
    "    \n",
    "remainder_val = X_val.shape[0] % (batch_size * timesteps)\n",
    "if remainder_val > 0:\n",
    "    X_val = np.concatenate([X_val, np.zeros((batch_size * timesteps - remainder_val, input_dim))])\n",
    "    y_val = np.concatenate([y_val, np.zeros((batch_size * timesteps - remainder_val,))])\n",
    "\n",
    "# Reshape the datasets to have the correct shape for the model\n",
    "X_train = X_train.reshape((-1, timesteps, input_dim))\n",
    "y_train = y_train.reshape((-1, timesteps))\n",
    "\n",
    "X_val = X_val.reshape((-1, timesteps, input_dim))\n",
    "y_val = y_val.reshape((-1, timesteps))\n",
    "\n",
    "# Change batch_size to be equal to X_train.shape[0]\n",
    "# batch_size = X_train.shape[0]\n",
    "print(X_train.shape[0])\n",
    "\n",
    "latent_dim = 2\n",
    "encoding_dim = 32\n",
    "\n",
    "# Set the input shape for the autoencoder model\n",
    "inputs = Input(batch_shape=(batch_size, timesteps, input_dim), name='encoder_input')\n",
    "x = LSTM(encoding_dim*2, return_sequences=True)(inputs) # Add LSTM layer with return_sequences set to True\n",
    "x = LSTM(encoding_dim, return_sequences=False)(x) # Add another LSTM layer with return_sequences set to False\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "# encoder.summary()\n",
    "\n",
    "# The decoder\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(encoding_dim, activation='relu')(latent_inputs)\n",
    "x = RepeatVector(timesteps)(x)\n",
    "x = LSTM(encoding_dim, return_sequences=True)(x)\n",
    "x = LSTM(encoding_dim*2, return_sequences=True)(x)\n",
    "outputs = TimeDistributed(Dense(input_dim))(x)\n",
    "outputs = Flatten()(outputs)\n",
    "# decoder_outputs = Dense(input_dim, activation='sigmoid')(x)\n",
    "\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "# decoder.summary()\n",
    "\n",
    "# vae_outputs = decoder(encoder(inputs)[2])\n",
    "vae_outputs = decoder(Lambda(sampling)([z_mean, z_log_var]))\n",
    "vae = Model(inputs, vae_outputs, name='vae')\n",
    "\n",
    "reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(vae_outputs))\n",
    "reconstruction_loss = K.mean(reconstruction_loss)\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.mean(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = reconstruction_loss + kl_loss\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "# vae.summary()\n",
    "\n",
    "# batch_size = X_train.shape[0]\n",
    "# num_samples = X_train.shape[0]\n",
    "# steps_per_epoch = num_samples // batch_size\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "history = vae.fit(X_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
    "\n",
    "# Use the encoder to generate embeddings for each sequence\n",
    "encoder_model = Model(inputs, z_mean)\n",
    "# print(encoder_model.layers[0].input_shape)\n",
    "print('The shape of the encoder model is: ', encoder_model.layers[0].input_shape)\n",
    "X_embedded = encoder_model.predict(X_train, batch_size=batch_size)\n",
    "# X_embedded = encoder_model.predict(X, batch_size=batch_size)\n",
    "\n",
    "# X_embedded = encoder_model.predict(X.reshape((int(X.shape[0]/timesteps), timesteps, input_dim)), batch_size=batch_size)\n",
    "\n",
    "# Train a classifier on the embeddings\n",
    "classifier = KMeans(n_clusters=14, random_state=0)\n",
    "y_pred = classifier.fit_predict(X_embedded)\n",
    "\n",
    "# Generate a fake dataset using the VAE model\n",
    "# n_samples = len(processed_data)\n",
    "# print(n_samples)\n",
    "# noise = np.random.normal(size=(n_samples, 5 - latent_dim))\n",
    "# noise = np.concatenate([noise, np.zeros((n_samples, latent_dim))], axis=-1)\n",
    "# # reshape noise to have the correct shape\n",
    "# noise = noise.reshape((int(noise.shape[0]/timesteps), timesteps, input_dim))\n",
    "# predicted_values = vae.predict(noise, batch_size=batch_size)\n",
    "\n",
    "# predicted_values = predicted_values.reshape((predicted_values.shape[0] * predicted_values.shape[1], predicted_values.shape[2]))\n",
    "\n",
    "# # undo the normalization\n",
    "# predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# # Round each of the values in the array to the nearest integer\n",
    "# predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# # Assign cluster labels to each of the predicted values\n",
    "# y_pred = kmeans.predict(predicted_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Older model code"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate a fake dataset using the VAE model\n",
    "n_samples = len(X_train)\n",
    "print(processed_data.shape)\n",
    "noise = np.random.normal(size=(n_samples, timesteps, input_dim))\n",
    "predicted_values = vae.predict(noise, batch_size=batch_size)\n",
    "# reshape predicted values to have the correct shape\n",
    "predicted_values = np.reshape(predicted_values, (n_samples*timesteps, input_dim))\n",
    "\n",
    "# undo the normalization\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Assign cluster labels to each of the predicted values\n",
    "y_pred = classifier.predict(encoder_model.predict(predicted_values, batch_size=batch_size))\n",
    "\n",
    "# Reshape y_pred to have the same shape as predicted_values\n",
    "y_pred = np.reshape(y_pred, (n_samples, timesteps))\n",
    "\n",
    "# Flatten the y_pred array to a 1D array of cluster labels\n",
    "y_pred = y_pred.flatten()\n",
    "\n",
    "# Create a new DataFrame with the desired column names and values\n",
    "predicted_data = pd.DataFrame.from_records(predicted_values, columns=['Timestamp', 'Device ID', 'Status', 'Activity', 'Activity Status'])\n",
    "predicted_data['Cluster'] = y_pred\n",
    "predicted_data = predicted_data.groupby('Timestamp', group_keys=False).apply(lambda x: x.sample(timesteps)).reset_index(drop=True)\n",
    "predicted_data.to_csv('Predictions/Aruba_17_prediction.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 4s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate a fake dataset using the VAE model\n",
    "n_samples = len(processed_data)\n",
    "\n",
    "noise = np.random.normal(size=(n_samples, timesteps, input_dim))\n",
    "predicted_values = vae.predict(noise, batch_size=batch_size)\n",
    "# reshape predicted values to have the correct shape\n",
    "predicted_values = np.reshape(predicted_values, (n_samples, timesteps, input_dim))\n",
    "\n",
    "# undo the normalization\n",
    "predicted_values = np.reshape(predicted_values, (-1, input_dim))\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Reshape predicted_values to match the input shape of encoder_model\n",
    "predicted_values = np.reshape(predicted_values, (n_samples, timesteps, input_dim))\n",
    "\n",
    "# # Assign cluster labels to each of the predicted values\n",
    "# y_pred = classifier.predict(encoder_model.predict(predicted_values, batch_size=batch_size))\n",
    "# # Print all information of the y_pred line above\n",
    "\n",
    "# # Reshape y_pred to match the shape of predicted_values\n",
    "# y_pred = np.reshape(y_pred, (n_samples, timesteps))\n",
    "\n",
    "# Save the prediction data to a new file 'predicted_Data.csv'\n",
    "predicted_data = pd.DataFrame(predicted_values.reshape((-1, input_dim)), columns=['Timestamp', 'Device ID', 'Status', 'Activity', 'Activity Status'])\n",
    "# predicted_data['Cluster'] = y_pred.reshape(-1)\n",
    "predicted_data.to_csv('Predictions/Aruba_17_prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e3f0318fa44a63fbd15a81336d0e6b9929111f70e7cf4cecf151c11d26f00aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
