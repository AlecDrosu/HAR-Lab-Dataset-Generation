{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, TimeDistributed, Reshape\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from sliding_window import read_data, segment_data_by_day, sliding_window\n",
    "# from custom_penalty import custom_penalty\n",
    "FILE_PATH = 'Processed Data/Aruba_17/processed_data.csv'\n",
    "\n",
    "data_df = read_data(FILE_PATH)\n",
    "daily_segments = segment_data_by_day(data_df)\n",
    "step_size = 100\n",
    "windows = sliding_window(daily_segments, step_size=step_size)\n",
    "\n",
    "# Prepare the data\n",
    "windows = np.asarray([window.to_numpy() for window in windows])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "timesteps = step_size\n",
    "input_dim = windows.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "428/428 [==============================] - 45s 85ms/step - loss: 977.4119 - val_loss: 831.7150\n",
      "Epoch 2/300\n",
      "428/428 [==============================] - 38s 89ms/step - loss: 874.9249 - val_loss: 811.8274\n",
      "Epoch 3/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 901.1291 - val_loss: 854.4156\n",
      "Epoch 4/300\n",
      "428/428 [==============================] - 37s 87ms/step - loss: 875.6805 - val_loss: 805.6607\n",
      "Epoch 5/300\n",
      "428/428 [==============================] - 37s 87ms/step - loss: 860.2598 - val_loss: 804.9722\n",
      "Epoch 6/300\n",
      "428/428 [==============================] - 37s 87ms/step - loss: 851.5999 - val_loss: 813.6790\n",
      "Epoch 7/300\n",
      "428/428 [==============================] - 37s 87ms/step - loss: 849.4973 - val_loss: 799.3619\n",
      "Epoch 8/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 851.8545 - val_loss: 799.9221\n",
      "Epoch 9/300\n",
      "428/428 [==============================] - 38s 89ms/step - loss: 846.4927 - val_loss: 799.1573\n",
      "Epoch 10/300\n",
      "428/428 [==============================] - 42s 97ms/step - loss: 846.0141 - val_loss: 798.5974\n",
      "Epoch 11/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 846.2817 - val_loss: 798.1117\n",
      "Epoch 12/300\n",
      "428/428 [==============================] - 40s 93ms/step - loss: 850.8247 - val_loss: 798.5086\n",
      "Epoch 13/300\n",
      "428/428 [==============================] - 40s 93ms/step - loss: 845.6714 - val_loss: 797.9759\n",
      "Epoch 14/300\n",
      "428/428 [==============================] - 40s 94ms/step - loss: 846.4368 - val_loss: 797.8367\n",
      "Epoch 15/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 845.6558 - val_loss: 799.2094\n",
      "Epoch 16/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 845.5382 - val_loss: 798.4930\n",
      "Epoch 17/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 845.9847 - val_loss: 798.2120\n",
      "Epoch 18/300\n",
      "428/428 [==============================] - 39s 90ms/step - loss: 846.8053 - val_loss: 797.8760\n",
      "Epoch 19/300\n",
      "428/428 [==============================] - 38s 88ms/step - loss: 848.4166 - val_loss: 799.7958\n",
      "Epoch 20/300\n",
      "428/428 [==============================] - 38s 89ms/step - loss: 845.8735 - val_loss: 799.5139\n",
      "Epoch 21/300\n",
      "428/428 [==============================] - 38s 89ms/step - loss: 1011.4657 - val_loss: 785.3013\n",
      "Epoch 22/300\n",
      "428/428 [==============================] - 38s 89ms/step - loss: 957.4404 - val_loss: 799.6962\n",
      "Epoch 23/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 845.7036 - val_loss: 799.6261\n",
      "Epoch 24/300\n",
      "428/428 [==============================] - 40s 94ms/step - loss: 846.4785 - val_loss: 802.6833\n",
      "Epoch 25/300\n",
      "428/428 [==============================] - 39s 90ms/step - loss: 847.8504 - val_loss: 800.8985\n",
      "Epoch 26/300\n",
      "428/428 [==============================] - 39s 90ms/step - loss: 847.0663 - val_loss: 799.5838\n",
      "Epoch 27/300\n",
      "428/428 [==============================] - 38s 89ms/step - loss: 846.3154 - val_loss: 797.1689\n",
      "Epoch 28/300\n",
      "428/428 [==============================] - 40s 93ms/step - loss: 845.5405 - val_loss: 797.6119\n",
      "Epoch 29/300\n",
      "428/428 [==============================] - 40s 93ms/step - loss: 845.4988 - val_loss: 796.9877\n",
      "Epoch 30/300\n",
      "428/428 [==============================] - 40s 93ms/step - loss: 845.4705 - val_loss: 797.0099\n",
      "Epoch 31/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 845.2758 - val_loss: 799.2610\n",
      "Epoch 32/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 845.4890 - val_loss: 797.7413\n",
      "Epoch 33/300\n",
      "428/428 [==============================] - 40s 94ms/step - loss: 846.3322 - val_loss: 798.8727\n",
      "Epoch 34/300\n",
      "428/428 [==============================] - 40s 94ms/step - loss: 845.4360 - val_loss: 798.5992\n",
      "Epoch 35/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 845.4662 - val_loss: 796.2547\n",
      "Epoch 36/300\n",
      "428/428 [==============================] - 40s 92ms/step - loss: 845.2650 - val_loss: 795.1844\n",
      "Epoch 37/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 845.3699 - val_loss: 796.2422\n",
      "Epoch 38/300\n",
      "428/428 [==============================] - 42s 98ms/step - loss: 845.5264 - val_loss: 798.0408\n",
      "Epoch 39/300\n",
      "428/428 [==============================] - 42s 99ms/step - loss: 845.4307 - val_loss: 797.5167\n",
      "Epoch 40/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 845.6141 - val_loss: 797.5905\n",
      "Epoch 41/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 845.0986 - val_loss: 797.4832\n",
      "Epoch 42/300\n",
      "428/428 [==============================] - 43s 99ms/step - loss: 846.8707 - val_loss: 798.0847\n",
      "Epoch 43/300\n",
      "428/428 [==============================] - 40s 94ms/step - loss: 845.4948 - val_loss: 798.1486\n",
      "Epoch 44/300\n",
      "428/428 [==============================] - 39s 90ms/step - loss: 845.0777 - val_loss: 797.4716\n",
      "Epoch 45/300\n",
      "428/428 [==============================] - 40s 94ms/step - loss: 845.3344 - val_loss: 797.2339\n",
      "Epoch 46/300\n",
      "428/428 [==============================] - 42s 98ms/step - loss: 844.9959 - val_loss: 798.3534\n",
      "Epoch 47/300\n",
      "428/428 [==============================] - 40s 94ms/step - loss: 845.3330 - val_loss: 797.3920\n",
      "Epoch 48/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 741.9896 - val_loss: 214.1043\n",
      "Epoch 49/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 202.4067 - val_loss: 172.7903\n",
      "Epoch 50/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 191.5854 - val_loss: 172.9459\n",
      "Epoch 51/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 190.1951 - val_loss: 170.8865\n",
      "Epoch 52/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 190.3993 - val_loss: 173.3194\n",
      "Epoch 53/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 189.9218 - val_loss: 172.6745\n",
      "Epoch 54/300\n",
      "428/428 [==============================] - 40s 93ms/step - loss: 189.8665 - val_loss: 172.6180\n",
      "Epoch 55/300\n",
      "428/428 [==============================] - 39s 90ms/step - loss: 189.8968 - val_loss: 173.4935\n",
      "Epoch 56/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 190.5484 - val_loss: 172.7300\n",
      "Epoch 57/300\n",
      "428/428 [==============================] - 40s 93ms/step - loss: 190.0843 - val_loss: 172.4794\n",
      "Epoch 58/300\n",
      "428/428 [==============================] - 40s 94ms/step - loss: 189.8688 - val_loss: 173.0266\n",
      "Epoch 59/300\n",
      "428/428 [==============================] - 39s 90ms/step - loss: 190.0599 - val_loss: 172.8714\n",
      "Epoch 60/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 189.9641 - val_loss: 172.9696\n",
      "Epoch 61/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 190.0517 - val_loss: 173.5659\n",
      "Epoch 62/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 189.8577 - val_loss: 172.4419\n",
      "Epoch 63/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 192.8824 - val_loss: 173.5497\n",
      "Epoch 64/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 190.1102 - val_loss: 173.6031\n",
      "Epoch 65/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 189.8481 - val_loss: 173.2689\n",
      "Epoch 66/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 189.8944 - val_loss: 173.2334\n",
      "Epoch 67/300\n",
      "428/428 [==============================] - 39s 90ms/step - loss: 190.1642 - val_loss: 172.7525\n",
      "Epoch 68/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 189.8273 - val_loss: 172.4511\n",
      "Epoch 69/300\n",
      "428/428 [==============================] - 40s 94ms/step - loss: 189.8237 - val_loss: 172.4574\n",
      "Epoch 70/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 189.8377 - val_loss: 172.2693\n",
      "Epoch 71/300\n",
      "428/428 [==============================] - 39s 90ms/step - loss: 189.8523 - val_loss: 171.2871\n",
      "Epoch 72/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 189.8349 - val_loss: 172.9371\n",
      "Epoch 73/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 189.9420 - val_loss: 173.2908\n",
      "Epoch 74/300\n",
      "428/428 [==============================] - 40s 92ms/step - loss: 190.3359 - val_loss: 173.0863\n",
      "Epoch 75/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 190.0536 - val_loss: 172.7292\n",
      "Epoch 76/300\n",
      "428/428 [==============================] - 40s 95ms/step - loss: 189.8460 - val_loss: 171.9366\n",
      "Epoch 77/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 189.8658 - val_loss: 173.4881\n",
      "Epoch 78/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 190.1416 - val_loss: 172.7118\n",
      "Epoch 79/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 189.9806 - val_loss: 172.5103\n",
      "Epoch 80/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 190.1207 - val_loss: 173.0145\n",
      "Epoch 81/300\n",
      "428/428 [==============================] - 40s 93ms/step - loss: 192.9690 - val_loss: 201.7302\n",
      "Epoch 82/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 247.6091 - val_loss: 219.1472\n",
      "Epoch 83/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 214.3096 - val_loss: 173.2377\n",
      "Epoch 84/300\n",
      "428/428 [==============================] - 40s 95ms/step - loss: 189.8890 - val_loss: 172.8572\n",
      "Epoch 85/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 189.9677 - val_loss: 172.1485\n",
      "Epoch 86/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 189.9055 - val_loss: 173.0641\n",
      "Epoch 87/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 189.7649 - val_loss: 171.9187\n",
      "Epoch 88/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 190.0982 - val_loss: 173.8345\n",
      "Epoch 89/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 189.8297 - val_loss: 171.9051\n",
      "Epoch 90/300\n",
      "428/428 [==============================] - 38s 90ms/step - loss: 189.7438 - val_loss: 172.1656\n",
      "Epoch 91/300\n",
      "428/428 [==============================] - 38s 89ms/step - loss: 189.7200 - val_loss: 173.2307\n",
      "Epoch 92/300\n",
      "428/428 [==============================] - 40s 93ms/step - loss: 189.9722 - val_loss: 172.5110\n",
      "Epoch 93/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 190.0764 - val_loss: 171.8987\n",
      "Epoch 94/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 189.8123 - val_loss: 172.6453\n",
      "Epoch 95/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 189.7687 - val_loss: 173.1649\n",
      "Epoch 96/300\n",
      "428/428 [==============================] - 40s 95ms/step - loss: 190.0365 - val_loss: 173.2190\n",
      "Epoch 97/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 190.0248 - val_loss: 172.7695\n",
      "Epoch 98/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 189.9273 - val_loss: 172.5134\n",
      "Epoch 99/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 188.7915 - val_loss: 166.6971\n",
      "Epoch 100/300\n",
      "428/428 [==============================] - 40s 93ms/step - loss: 190.0275 - val_loss: 165.7508\n",
      "Epoch 101/300\n",
      "428/428 [==============================] - 39s 90ms/step - loss: 184.2697 - val_loss: 164.6470\n",
      "Epoch 102/300\n",
      "428/428 [==============================] - 39s 90ms/step - loss: 183.8994 - val_loss: 165.0355\n",
      "Epoch 103/300\n",
      "428/428 [==============================] - 39s 90ms/step - loss: 183.5415 - val_loss: 163.5044\n",
      "Epoch 104/300\n",
      "428/428 [==============================] - 38s 90ms/step - loss: 183.2510 - val_loss: 163.7524\n",
      "Epoch 105/300\n",
      "428/428 [==============================] - 38s 89ms/step - loss: 183.1485 - val_loss: 164.0544\n",
      "Epoch 106/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 183.1022 - val_loss: 163.6943\n",
      "Epoch 107/300\n",
      "428/428 [==============================] - 40s 93ms/step - loss: 183.3270 - val_loss: 163.1306\n",
      "Epoch 108/300\n",
      "428/428 [==============================] - 38s 90ms/step - loss: 183.1701 - val_loss: 163.3331\n",
      "Epoch 109/300\n",
      "428/428 [==============================] - 39s 90ms/step - loss: 183.1322 - val_loss: 163.4575\n",
      "Epoch 110/300\n",
      "428/428 [==============================] - 39s 91ms/step - loss: 183.3420 - val_loss: 164.8187\n",
      "Epoch 111/300\n",
      "428/428 [==============================] - 39s 92ms/step - loss: 183.0042 - val_loss: 164.1539\n",
      "Epoch 112/300\n",
      "428/428 [==============================] - 45s 104ms/step - loss: 183.0290 - val_loss: 164.6863\n",
      "Epoch 113/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 182.9058 - val_loss: 164.3144\n",
      "Epoch 114/300\n",
      "428/428 [==============================] - 46s 107ms/step - loss: 182.8329 - val_loss: 164.0516\n",
      "Epoch 115/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 182.5932 - val_loss: 164.5890\n",
      "Epoch 116/300\n",
      "428/428 [==============================] - 42s 98ms/step - loss: 182.5200 - val_loss: 163.5385\n",
      "Epoch 117/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 182.4111 - val_loss: 163.0779\n",
      "Epoch 118/300\n",
      "428/428 [==============================] - 43s 101ms/step - loss: 182.8459 - val_loss: 165.6330\n",
      "Epoch 119/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 182.6085 - val_loss: 166.1136\n",
      "Epoch 120/300\n",
      "428/428 [==============================] - 43s 99ms/step - loss: 182.6781 - val_loss: 163.8463\n",
      "Epoch 121/300\n",
      "428/428 [==============================] - 44s 102ms/step - loss: 182.3273 - val_loss: 163.6721\n",
      "Epoch 122/300\n",
      "428/428 [==============================] - 43s 101ms/step - loss: 182.2504 - val_loss: 164.4772\n",
      "Epoch 123/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 182.2721 - val_loss: 163.4341\n",
      "Epoch 124/300\n",
      "428/428 [==============================] - 44s 103ms/step - loss: 182.1617 - val_loss: 163.3238\n",
      "Epoch 125/300\n",
      "428/428 [==============================] - 44s 102ms/step - loss: 182.1304 - val_loss: 163.4510\n",
      "Epoch 126/300\n",
      "428/428 [==============================] - 42s 97ms/step - loss: 182.3913 - val_loss: 163.5898\n",
      "Epoch 127/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 182.1414 - val_loss: 163.8724\n",
      "Epoch 128/300\n",
      "428/428 [==============================] - 42s 98ms/step - loss: 182.3876 - val_loss: 163.6516\n",
      "Epoch 129/300\n",
      "428/428 [==============================] - 41s 97ms/step - loss: 182.1806 - val_loss: 164.2102\n",
      "Epoch 130/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 182.1695 - val_loss: 164.0669\n",
      "Epoch 131/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 182.0584 - val_loss: 164.2059\n",
      "Epoch 132/300\n",
      "428/428 [==============================] - 40s 95ms/step - loss: 182.0237 - val_loss: 163.5271\n",
      "Epoch 133/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 195.1444 - val_loss: 176.7849\n",
      "Epoch 134/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 184.8358 - val_loss: 166.6243\n",
      "Epoch 135/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 183.3728 - val_loss: 165.4656\n",
      "Epoch 136/300\n",
      "428/428 [==============================] - 42s 99ms/step - loss: 183.2256 - val_loss: 166.3098\n",
      "Epoch 137/300\n",
      "428/428 [==============================] - 41s 97ms/step - loss: 181.1278 - val_loss: 166.1963\n",
      "Epoch 138/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 180.0070 - val_loss: 164.7879\n",
      "Epoch 139/300\n",
      "428/428 [==============================] - 41s 97ms/step - loss: 180.1330 - val_loss: 164.7711\n",
      "Epoch 140/300\n",
      "428/428 [==============================] - 41s 97ms/step - loss: 179.6171 - val_loss: 164.1193\n",
      "Epoch 141/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.2054 - val_loss: 165.0748\n",
      "Epoch 142/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 178.8418 - val_loss: 165.3645\n",
      "Epoch 143/300\n",
      "428/428 [==============================] - 43s 99ms/step - loss: 178.7418 - val_loss: 164.5840\n",
      "Epoch 144/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 180.9485 - val_loss: 164.5493\n",
      "Epoch 145/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.1124 - val_loss: 162.9973\n",
      "Epoch 146/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.1388 - val_loss: 164.3864\n",
      "Epoch 147/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 178.6612 - val_loss: 163.1055\n",
      "Epoch 148/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 178.4344 - val_loss: 163.5730\n",
      "Epoch 149/300\n",
      "428/428 [==============================] - 41s 97ms/step - loss: 178.6839 - val_loss: 164.7351\n",
      "Epoch 150/300\n",
      "428/428 [==============================] - 42s 99ms/step - loss: 178.3883 - val_loss: 163.0786\n",
      "Epoch 151/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 178.3270 - val_loss: 163.2586\n",
      "Epoch 152/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.4682 - val_loss: 164.5761\n",
      "Epoch 153/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.1744 - val_loss: 164.0162\n",
      "Epoch 154/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 178.5831 - val_loss: 164.3323\n",
      "Epoch 155/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.5404 - val_loss: 165.0635\n",
      "Epoch 156/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 180.3858 - val_loss: 163.6915\n",
      "Epoch 157/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 178.5366 - val_loss: 163.3085\n",
      "Epoch 158/300\n",
      "428/428 [==============================] - 42s 99ms/step - loss: 178.7837 - val_loss: 163.3023\n",
      "Epoch 159/300\n",
      "428/428 [==============================] - 43s 101ms/step - loss: 178.4386 - val_loss: 163.5839\n",
      "Epoch 160/300\n",
      "428/428 [==============================] - 42s 98ms/step - loss: 178.2585 - val_loss: 163.3218\n",
      "Epoch 161/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 178.8687 - val_loss: 164.0558\n",
      "Epoch 162/300\n",
      "428/428 [==============================] - 41s 97ms/step - loss: 178.3648 - val_loss: 162.4234\n",
      "Epoch 163/300\n",
      "428/428 [==============================] - 42s 97ms/step - loss: 178.1189 - val_loss: 162.4972\n",
      "Epoch 164/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 177.9838 - val_loss: 162.8875\n",
      "Epoch 165/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 178.6534 - val_loss: 163.8669\n",
      "Epoch 166/300\n",
      "428/428 [==============================] - 41s 97ms/step - loss: 178.3591 - val_loss: 162.7139\n",
      "Epoch 167/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 178.0528 - val_loss: 162.1705\n",
      "Epoch 168/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 177.9532 - val_loss: 162.2066\n",
      "Epoch 169/300\n",
      "428/428 [==============================] - 42s 98ms/step - loss: 178.5363 - val_loss: 163.4007\n",
      "Epoch 170/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 190.4203 - val_loss: 169.1641\n",
      "Epoch 171/300\n",
      "428/428 [==============================] - 42s 97ms/step - loss: 182.5451 - val_loss: 164.7801\n",
      "Epoch 172/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 181.3326 - val_loss: 164.3530\n",
      "Epoch 173/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 181.1309 - val_loss: 165.4398\n",
      "Epoch 174/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 184.1127 - val_loss: 179.5004\n",
      "Epoch 175/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 183.9095 - val_loss: 165.0098\n",
      "Epoch 176/300\n",
      "428/428 [==============================] - 42s 97ms/step - loss: 183.5102 - val_loss: 164.4345\n",
      "Epoch 177/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 180.7887 - val_loss: 164.3611\n",
      "Epoch 178/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 182.0875 - val_loss: 164.5023\n",
      "Epoch 179/300\n",
      "428/428 [==============================] - 42s 99ms/step - loss: 180.0794 - val_loss: 163.3110\n",
      "Epoch 180/300\n",
      "428/428 [==============================] - 41s 97ms/step - loss: 180.5711 - val_loss: 164.4018\n",
      "Epoch 181/300\n",
      "428/428 [==============================] - 41s 97ms/step - loss: 179.5190 - val_loss: 164.5387\n",
      "Epoch 182/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 182.2835 - val_loss: 168.8843\n",
      "Epoch 183/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 184.6888 - val_loss: 166.7091\n",
      "Epoch 184/300\n",
      "428/428 [==============================] - 41s 97ms/step - loss: 182.8138 - val_loss: 165.1507\n",
      "Epoch 185/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 181.2733 - val_loss: 165.6148\n",
      "Epoch 186/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 180.9339 - val_loss: 164.3631\n",
      "Epoch 187/300\n",
      "428/428 [==============================] - 43s 101ms/step - loss: 180.3359 - val_loss: 164.0335\n",
      "Epoch 188/300\n",
      "428/428 [==============================] - 40s 94ms/step - loss: 179.7531 - val_loss: 163.8505\n",
      "Epoch 189/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 179.5813 - val_loss: 164.7564\n",
      "Epoch 190/300\n",
      "428/428 [==============================] - 41s 95ms/step - loss: 179.4119 - val_loss: 164.2263\n",
      "Epoch 191/300\n",
      "428/428 [==============================] - 42s 97ms/step - loss: 184.6033 - val_loss: 166.4251\n",
      "Epoch 192/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 181.9694 - val_loss: 164.9373\n",
      "Epoch 193/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 181.5237 - val_loss: 165.4950\n",
      "Epoch 194/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 180.0278 - val_loss: 163.7984\n",
      "Epoch 195/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.9894 - val_loss: 163.9622\n",
      "Epoch 196/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.7245 - val_loss: 164.2868\n",
      "Epoch 197/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.4646 - val_loss: 163.7594\n",
      "Epoch 198/300\n",
      "428/428 [==============================] - 41s 97ms/step - loss: 181.9781 - val_loss: 165.2054\n",
      "Epoch 199/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 180.3608 - val_loss: 164.3819\n",
      "Epoch 200/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.2204 - val_loss: 164.0822\n",
      "Epoch 201/300\n",
      "428/428 [==============================] - 42s 99ms/step - loss: 179.2012 - val_loss: 163.5181\n",
      "Epoch 202/300\n",
      "428/428 [==============================] - 42s 97ms/step - loss: 179.4784 - val_loss: 163.2635\n",
      "Epoch 203/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 178.9846 - val_loss: 163.4992\n",
      "Epoch 204/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.5144 - val_loss: 163.6614\n",
      "Epoch 205/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 178.9147 - val_loss: 163.1710\n",
      "Epoch 206/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.1684 - val_loss: 165.5271\n",
      "Epoch 207/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 178.7305 - val_loss: 164.7740\n",
      "Epoch 208/300\n",
      "428/428 [==============================] - 42s 98ms/step - loss: 179.7540 - val_loss: 166.7356\n",
      "Epoch 209/300\n",
      "428/428 [==============================] - 42s 98ms/step - loss: 179.8000 - val_loss: 164.3317\n",
      "Epoch 210/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.4386 - val_loss: 163.6552\n",
      "Epoch 211/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.7240 - val_loss: 163.5294\n",
      "Epoch 212/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.3151 - val_loss: 165.2421\n",
      "Epoch 213/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 179.1546 - val_loss: 171.6512\n",
      "Epoch 214/300\n",
      "428/428 [==============================] - 46s 108ms/step - loss: 179.2341 - val_loss: 164.3604\n",
      "Epoch 215/300\n",
      "428/428 [==============================] - 44s 102ms/step - loss: 179.2027 - val_loss: 165.0723\n",
      "Epoch 216/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 178.8351 - val_loss: 163.2509\n",
      "Epoch 217/300\n",
      "428/428 [==============================] - 53s 124ms/step - loss: 190.6829 - val_loss: 171.6276\n",
      "Epoch 218/300\n",
      "428/428 [==============================] - 42s 98ms/step - loss: 186.2731 - val_loss: 165.3546\n",
      "Epoch 219/300\n",
      "428/428 [==============================] - 43s 101ms/step - loss: 181.7652 - val_loss: 164.8473\n",
      "Epoch 220/300\n",
      "428/428 [==============================] - 42s 99ms/step - loss: 180.5436 - val_loss: 164.0931\n",
      "Epoch 221/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 180.2355 - val_loss: 164.3997\n",
      "Epoch 222/300\n",
      "428/428 [==============================] - 43s 101ms/step - loss: 179.8929 - val_loss: 164.5134\n",
      "Epoch 223/300\n",
      "428/428 [==============================] - 46s 108ms/step - loss: 180.5863 - val_loss: 163.1637\n",
      "Epoch 224/300\n",
      "428/428 [==============================] - 45s 106ms/step - loss: 179.8748 - val_loss: 163.9688\n",
      "Epoch 225/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 179.5064 - val_loss: 168.0228\n",
      "Epoch 226/300\n",
      "428/428 [==============================] - 44s 103ms/step - loss: 179.3439 - val_loss: 164.6562\n",
      "Epoch 227/300\n",
      "428/428 [==============================] - 46s 107ms/step - loss: 185.1140 - val_loss: 166.1875\n",
      "Epoch 228/300\n",
      "428/428 [==============================] - 43s 100ms/step - loss: 190.6433 - val_loss: 166.0263\n",
      "Epoch 229/300\n",
      "428/428 [==============================] - 48s 112ms/step - loss: 184.1982 - val_loss: 164.1042\n",
      "Epoch 230/300\n",
      "428/428 [==============================] - 48s 113ms/step - loss: 182.0044 - val_loss: 165.7016\n",
      "Epoch 231/300\n",
      "428/428 [==============================] - 48s 113ms/step - loss: 180.9609 - val_loss: 163.9125\n",
      "Epoch 232/300\n",
      "428/428 [==============================] - 49s 114ms/step - loss: 180.7195 - val_loss: 163.6529\n",
      "Epoch 233/300\n",
      "428/428 [==============================] - 48s 112ms/step - loss: 179.7025 - val_loss: 163.0927\n",
      "Epoch 234/300\n",
      "428/428 [==============================] - 47s 111ms/step - loss: 179.1146 - val_loss: 164.7966\n",
      "Epoch 235/300\n",
      "428/428 [==============================] - 46s 108ms/step - loss: 191.2863 - val_loss: 165.4160\n",
      "Epoch 236/300\n",
      "428/428 [==============================] - 45s 106ms/step - loss: 184.8513 - val_loss: 165.3809\n",
      "Epoch 237/300\n",
      "428/428 [==============================] - 51s 119ms/step - loss: 182.2411 - val_loss: 164.7307\n",
      "Epoch 238/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 191.7297 - val_loss: 167.9774\n",
      "Epoch 239/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 182.1920 - val_loss: 164.3401\n",
      "Epoch 240/300\n",
      "428/428 [==============================] - 53s 123ms/step - loss: 181.3329 - val_loss: 164.0664\n",
      "Epoch 241/300\n",
      "428/428 [==============================] - 53s 124ms/step - loss: 180.7210 - val_loss: 164.0164\n",
      "Epoch 242/300\n",
      "428/428 [==============================] - 53s 123ms/step - loss: 180.1805 - val_loss: 165.5681\n",
      "Epoch 243/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 180.4953 - val_loss: 168.2435\n",
      "Epoch 244/300\n",
      "428/428 [==============================] - 53s 123ms/step - loss: 180.9711 - val_loss: 163.8103\n",
      "Epoch 245/300\n",
      "428/428 [==============================] - 53s 123ms/step - loss: 179.5325 - val_loss: 163.7186\n",
      "Epoch 246/300\n",
      "428/428 [==============================] - 51s 120ms/step - loss: 180.0090 - val_loss: 163.9786\n",
      "Epoch 247/300\n",
      "428/428 [==============================] - 54s 126ms/step - loss: 180.3717 - val_loss: 163.6999\n",
      "Epoch 248/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 179.4134 - val_loss: 163.2881\n",
      "Epoch 249/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 191.1968 - val_loss: 174.0060\n",
      "Epoch 250/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 188.1943 - val_loss: 167.8153\n",
      "Epoch 251/300\n",
      "428/428 [==============================] - 52s 122ms/step - loss: 182.6759 - val_loss: 166.6497\n",
      "Epoch 252/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 180.5139 - val_loss: 165.2024\n",
      "Epoch 253/300\n",
      "428/428 [==============================] - 53s 125ms/step - loss: 179.7735 - val_loss: 163.9924\n",
      "Epoch 254/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 179.2390 - val_loss: 164.6747\n",
      "Epoch 255/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 179.0673 - val_loss: 163.8115\n",
      "Epoch 256/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 178.9174 - val_loss: 163.9486\n",
      "Epoch 257/300\n",
      "428/428 [==============================] - 52s 122ms/step - loss: 179.3688 - val_loss: 164.4734\n",
      "Epoch 258/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 179.0149 - val_loss: 163.0814\n",
      "Epoch 259/300\n",
      "428/428 [==============================] - 54s 127ms/step - loss: 178.9352 - val_loss: 163.6593\n",
      "Epoch 260/300\n",
      "428/428 [==============================] - 52s 120ms/step - loss: 178.5191 - val_loss: 163.5070\n",
      "Epoch 261/300\n",
      "428/428 [==============================] - 52s 122ms/step - loss: 178.2322 - val_loss: 162.9403\n",
      "Epoch 262/300\n",
      "428/428 [==============================] - 52s 122ms/step - loss: 178.4846 - val_loss: 163.2014\n",
      "Epoch 263/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 178.4632 - val_loss: 163.3963\n",
      "Epoch 264/300\n",
      "428/428 [==============================] - 51s 120ms/step - loss: 178.1956 - val_loss: 164.6434\n",
      "Epoch 265/300\n",
      "428/428 [==============================] - 54s 126ms/step - loss: 178.0497 - val_loss: 162.7672\n",
      "Epoch 266/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 180.1053 - val_loss: 165.3071\n",
      "Epoch 267/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 179.0368 - val_loss: 164.5728\n",
      "Epoch 268/300\n",
      "428/428 [==============================] - 52s 120ms/step - loss: 178.3724 - val_loss: 163.5458\n",
      "Epoch 269/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 178.1410 - val_loss: 163.6558\n",
      "Epoch 270/300\n",
      "428/428 [==============================] - 53s 124ms/step - loss: 178.0158 - val_loss: 163.4054\n",
      "Epoch 271/300\n",
      "428/428 [==============================] - 53s 123ms/step - loss: 178.3110 - val_loss: 163.9432\n",
      "Epoch 272/300\n",
      "428/428 [==============================] - 51s 120ms/step - loss: 178.2449 - val_loss: 163.3721\n",
      "Epoch 273/300\n",
      "428/428 [==============================] - 53s 123ms/step - loss: 179.1189 - val_loss: 163.1952\n",
      "Epoch 274/300\n",
      "428/428 [==============================] - 51s 119ms/step - loss: 178.1320 - val_loss: 162.6621\n",
      "Epoch 275/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 177.9938 - val_loss: 163.4260\n",
      "Epoch 276/300\n",
      "428/428 [==============================] - 53s 125ms/step - loss: 177.9683 - val_loss: 163.4667\n",
      "Epoch 277/300\n",
      "428/428 [==============================] - 52s 122ms/step - loss: 177.9196 - val_loss: 162.4496\n",
      "Epoch 278/300\n",
      "428/428 [==============================] - 51s 120ms/step - loss: 178.9593 - val_loss: 162.7723\n",
      "Epoch 279/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 177.8758 - val_loss: 164.0323\n",
      "Epoch 280/300\n",
      "428/428 [==============================] - 52s 121ms/step - loss: 179.0015 - val_loss: 165.4713\n",
      "Epoch 281/300\n",
      "428/428 [==============================] - 52s 122ms/step - loss: 178.4016 - val_loss: 164.6472\n",
      "Epoch 282/300\n",
      "428/428 [==============================] - 53s 124ms/step - loss: 178.1842 - val_loss: 163.7238\n",
      "Epoch 283/300\n",
      "428/428 [==============================] - 51s 120ms/step - loss: 177.8506 - val_loss: 163.6079\n",
      "Epoch 284/300\n",
      "428/428 [==============================] - 55s 128ms/step - loss: 177.7793 - val_loss: 163.2573\n",
      "Epoch 285/300\n",
      "428/428 [==============================] - 49s 116ms/step - loss: 177.8114 - val_loss: 163.4070\n",
      "Epoch 286/300\n",
      "428/428 [==============================] - 42s 98ms/step - loss: 179.7195 - val_loss: 163.3866\n",
      "Epoch 287/300\n",
      "428/428 [==============================] - 43s 99ms/step - loss: 178.2589 - val_loss: 164.7090\n",
      "Epoch 288/300\n",
      "428/428 [==============================] - 47s 109ms/step - loss: 178.0806 - val_loss: 164.3895\n",
      "Epoch 289/300\n",
      "428/428 [==============================] - 46s 107ms/step - loss: 178.1180 - val_loss: 162.6832\n",
      "Epoch 290/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 177.9256 - val_loss: 162.8434\n",
      "Epoch 291/300\n",
      "428/428 [==============================] - 45s 106ms/step - loss: 179.2211 - val_loss: 163.1137\n",
      "Epoch 292/300\n",
      "428/428 [==============================] - 46s 107ms/step - loss: 178.0780 - val_loss: 162.1407\n",
      "Epoch 293/300\n",
      "428/428 [==============================] - 46s 107ms/step - loss: 177.8489 - val_loss: 162.6095\n",
      "Epoch 294/300\n",
      "428/428 [==============================] - 42s 99ms/step - loss: 177.7834 - val_loss: 162.7345\n",
      "Epoch 295/300\n",
      "428/428 [==============================] - 44s 102ms/step - loss: 177.5643 - val_loss: 163.7186\n",
      "Epoch 296/300\n",
      "428/428 [==============================] - 46s 107ms/step - loss: 177.6229 - val_loss: 163.0259\n",
      "Epoch 297/300\n",
      "428/428 [==============================] - 43s 101ms/step - loss: 177.7518 - val_loss: 162.9393\n",
      "Epoch 298/300\n",
      "428/428 [==============================] - 42s 98ms/step - loss: 178.4964 - val_loss: 163.2856\n",
      "Epoch 299/300\n",
      "428/428 [==============================] - 42s 98ms/step - loss: 178.3436 - val_loss: 163.0546\n",
      "Epoch 300/300\n",
      "428/428 [==============================] - 41s 96ms/step - loss: 177.7238 - val_loss: 163.3236\n",
      "428/428 [==============================] - 8s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "# Create a new train-test split using the windows\n",
    "window_train, window_val = train_test_split(windows, test_size=validation_split, shuffle=False)\n",
    "\n",
    "# Prepare the input data for the model by concatenating the windows along the time axis\n",
    "X_train = np.concatenate(window_train, axis=0)\n",
    "X_val = np.concatenate(window_val, axis=0)\n",
    "\n",
    "# Normalize the data using minMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train = X_train.reshape((-1, timesteps, input_dim))\n",
    "X_val = X_val.reshape((-1, timesteps, input_dim))\n",
    "\n",
    "latent_dim = 2\n",
    "encoding_dim = 32\n",
    "\n",
    "# ==================== ENCODER ====================\n",
    "inputs = Input(shape=(timesteps, input_dim), name='encoder_input')\n",
    "x = LSTM(encoding_dim*2, return_sequences=True)(inputs)\n",
    "x = LSTM(encoding_dim, return_sequences=False)(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "# z_mean is the mean of the latent space\n",
    "# z_log_var is the variance of the latent space\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# ================= LATENT SPACE ==================\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "# ==================== DECODER ====================\n",
    "x = Dense(timesteps * encoding_dim, activation='relu')(latent_inputs)\n",
    "x = Reshape((timesteps, encoding_dim))(x)\n",
    "x = LSTM(encoding_dim, return_sequences=True, input_shape=(timesteps, encoding_dim))(x)\n",
    "x = TimeDistributed(Dense(input_dim))(x)\n",
    "# LSTM layer in the decoder is used to reconstruct the original sequence\n",
    "\n",
    "# the VAE model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = Model(latent_inputs, x, name='decoder')\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "# OLD LOSS FUNCTION -------------------------------------------------------------------------------------------------------------#\n",
    "reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n",
    "reconstruction_loss *= timesteps * input_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "# ------------------------------------------------------------------------------------------------------------------------------#\n",
    "num_epochs = 300\n",
    "history = vae.fit(X_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, None))\n",
    "\n",
    "# Use the encoder to generate embeddings for each sequence\n",
    "encoder_model = Model(inputs, z_mean)\n",
    "\n",
    "# Use the encoder to generate embeddings for each sequence\n",
    "encoder_model = Model(inputs, z_mean)\n",
    "X_embedded = encoder_model.predict(X_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535/535 [==============================] - 6s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate a fake dataset using the VAE model\n",
    "n_samples = len(windows)\n",
    "\n",
    "# Sample from the latent space\n",
    "z_samples = np.random.normal(size=(n_samples, latent_dim))\n",
    "\n",
    "# Use the decoder to generate the output\n",
    "predicted_values = decoder.predict(z_samples)\n",
    "predicted_values = np.reshape(predicted_values, (n_samples, timesteps, input_dim))\n",
    "\n",
    "# Undo the normalization\n",
    "predicted_values = np.reshape(predicted_values, (-1, input_dim))\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Create the fake dataset in the original format\n",
    "fake_dataset = []\n",
    "for window in predicted_values.reshape((n_samples, timesteps, input_dim)):\n",
    "    fake_dataset.extend(window)\n",
    "\n",
    "# Save the fake dataset to a new file 'fake_dataset.txt'\n",
    "fake_data = pd.DataFrame(fake_dataset, columns=['Date', 'Time', 'Device ID', 'Status', 'Activity', 'Activity Status'])\n",
    "\n",
    "with open('fake_dataset.txt', 'w') as file:\n",
    "    for _, row in fake_data.iterrows():\n",
    "        file.write(','.join(map(str, row.values)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss with x and y labels, and a grid\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "# Validation loss > training loss, underfitting\n",
    "# validation loss > training loss, overfitting, if it decreases and then increases again.\n",
    "# If they both decreease and stabilize at a specific point, it is an optimal fit.\n",
    "\n",
    "# Plot the evaluation loss vs the iterations\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the model\n",
    "# from keras.utils import plot_model\n",
    "\n",
    "# # Display the layers, number of layers, number of nodes etc\n",
    "# plot_model(vae, to_file='vae.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# # Load the image and display it\n",
    "# img = plt.imread('vae.png')\n",
    "# plt.figure(figsize=(16, 12))\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e3f0318fa44a63fbd15a81336d0e6b9929111f70e7cf4cecf151c11d26f00aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
