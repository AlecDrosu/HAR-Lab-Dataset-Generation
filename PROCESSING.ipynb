{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing code\n",
    "This code will convert the original data file into a csv for the Aruba dataset. Saved as pre_processed_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the input file for reading\n",
    "with open(\"Raw Data/Aruba_17/data\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "    # only read the first 1000 lines\n",
    "\n",
    "# Create an empty list to store the processed data\n",
    "processed_data = []\n",
    "\n",
    "# Possible activities\n",
    "activities = [\"Meal_Preparation\", \"Relax\", \"Eating\", \"Work\", \"Sleeping\", \"Wash_Dishes\", \"Bed_to_Toilet\", \"Enter_Home\", \"Leave_Home\", \"Housekeeping\", \"Respirate\"]\n",
    "\n",
    "# Loop through each line of the data\n",
    "for line in data:\n",
    "    # Split the line into its components\n",
    "    components = re.split(\"\\s+\", line.strip())\n",
    "\n",
    "    date = components[0]\n",
    "    time = components[1]\n",
    "    device_id = components[2]\n",
    "    device_status = components[3]\n",
    "    if len(components) > 4:\n",
    "        activity = components[4]\n",
    "        activity_status = components[5]\n",
    "\n",
    "    try:\n",
    "        timestamp = datetime.datetime.strptime(f'{date} {time}', '%Y-%m-%d %H:%M:%S.%f')\n",
    "        timestamp = int(timestamp.timestamp())\n",
    "\n",
    "    except ValueError:\n",
    "        timestamp = datetime.datetime.strptime(f'{date} {time}', '%Y-%m-%d %H:%M:%S')\n",
    "        timestamp = int(timestamp.timestamp())\n",
    "\n",
    "    if device_id.startswith(\"M\"):\n",
    "        if device_status.startswith(\"ON\"):\n",
    "            device_status = \"ON\"\n",
    "        elif device_status.startswith(\"OFF\"):\n",
    "            device_status = \"OFF\"\n",
    "\n",
    "    # Append the processed data to the list\n",
    "    if len(components) > 4:\n",
    "        processed_data.append([timestamp, device_id, device_status, activity, activity_status])\n",
    "    else:\n",
    "        processed_data.append([timestamp, device_id, device_status, \"\", \"\"])\n",
    "\n",
    "# Write the processed data to a new file\n",
    "with open(\"Processed Data/Aruba_17/pre_processed_data.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Timestamp\", \"Device ID\", \"Status\", \"Activity\", \"Activity Status\"])\n",
    "    for data in processed_data:\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Processing Code\n",
    "The original data is saved in a way that the model cannot use. The model needs the data to be numerical. This code will convert the previous file into a csv that the model can be trained on. Saved as processed_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Load the processed data file\n",
    "data = pd.read_csv(\"Processed Data/Aruba_17/pre_processed_data.csv\")\n",
    "\n",
    "# Encode the following columns: Timestamp,Device ID,Status,Activity,Activity Status\n",
    "timestamp_encoder = LabelEncoder()\n",
    "device_id_encoder = LabelEncoder()\n",
    "status_encoder = LabelEncoder()\n",
    "activity_encoder = LabelEncoder()\n",
    "activity_status_encoder = LabelEncoder()\n",
    "\n",
    "timestamp_encoder.fit(data['Timestamp'])\n",
    "device_id_encoder.fit(data['Device ID'])\n",
    "status_encoder.fit(data['Status'])\n",
    "activity_encoder.fit(data['Activity'])\n",
    "activity_status_encoder.fit(data['Activity Status'])\n",
    "\n",
    "data['Timestamp'] = timestamp_encoder.transform(data['Timestamp'])\n",
    "data['Device ID'] = device_id_encoder.transform(data['Device ID'])\n",
    "data['Status'] = status_encoder.transform(data['Status'])\n",
    "data['Activity'] = activity_encoder.transform(data['Activity'])\n",
    "data['Activity Status'] = activity_status_encoder.transform(data['Activity Status'])\n",
    "\n",
    "data.to_csv('Processed Data/Aruba_17/processed_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Post-processing Code\n",
    "This code will inport the prediction data from the model. It will then convert the Label Encoded data back into the original labels. Saved as COMPLETE_PREDICTION.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mPredictions/Aruba_17_prediction.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39m# data = pd.read_csv(\"Processed Data/Aruba_17/processed_data.csv\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[39m# use inverse_transform to get the original values\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mTimestamp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m timestamp_encoder\u001b[39m.\u001b[39;49minverse_transform(data[\u001b[39m'\u001b[39;49m\u001b[39mTimestamp\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      7\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mDevice ID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m device_id_encoder\u001b[39m.\u001b[39minverse_transform(data[\u001b[39m'\u001b[39m\u001b[39mDevice ID\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mStatus\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m status_encoder\u001b[39m.\u001b[39minverse_transform(data[\u001b[39m'\u001b[39m\u001b[39mStatus\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\preprocessing\\_label.py:163\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my contains previously unseen labels: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(diff))\n\u001b[0;32m    162\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(y)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses_[y]\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Predictions/Aruba_17_prediction.csv\")\n",
    "# data = pd.read_csv(\"Processed Data/Aruba_17/processed_data.csv\")\n",
    "\n",
    "# use inverse_transform to get the original values\n",
    "\n",
    "data['Timestamp'] = timestamp_encoder.inverse_transform(data['Timestamp'])\n",
    "data['Device ID'] = device_id_encoder.inverse_transform(data['Device ID'])\n",
    "data['Status'] = status_encoder.inverse_transform(data['Status'])\n",
    "data['Activity'] = activity_encoder.inverse_transform(data['Activity'])\n",
    "data['Activity Status'] = activity_status_encoder.inverse_transform(data['Activity Status'])\n",
    "\n",
    "# undo the timestamp code to get the original two columns, date and time, in the following form: 2010-11-04 00:03:50.209589\n",
    "data['Timestamp'] = data['Timestamp'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "\n",
    "# split the timestamp column into two columns, date and time\n",
    "data[['Date', 'Time']] = data['Timestamp'].str.split(' ', 1, expand=True)\n",
    "\n",
    "# drop the timestamp column\n",
    "data = data.drop(columns=['Timestamp'])\n",
    "\n",
    "# reorder the columns\n",
    "data = data[['Date', 'Time', 'Device ID', 'Status', 'Activity', 'Activity Status']]\n",
    "\n",
    "# save the data to a new file\n",
    "data.to_csv('Predictions/Aruba_17_completed_prediction.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e3f0318fa44a63fbd15a81336d0e6b9929111f70e7cf4cecf151c11d26f00aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
