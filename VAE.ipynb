{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 32)           192         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            66          ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            66          ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 324\n",
      "Trainable params: 324\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 32)                96        \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261\n",
      "Trainable params: 261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " encoder (Functional)           [(None, 2),          324         ['encoder_input[0][0]']          \n",
      "                                 (None, 2),                                                       \n",
      "                                 (None, 2)]                                                       \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 5)            261         ['encoder[0][2]']                \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 32)           192         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            66          ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            66          ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 2)           0           ['z_log_var[0][0]']              \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.square_10 (TFOpLambda)  (None, 2)           0           ['z_mean[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.cast_14 (TFOpLambda)        (None, 5)            0           ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " tf.convert_to_tensor_24 (TFOpL  (None, 5)           0           ['decoder[0][0]']                \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.subtract_20 (TFOpLambd  (None, 2)           0           ['tf.__operators__.add_20[0][0]',\n",
      " a)                                                               'tf.math.square_10[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.exp_10 (TFOpLambda)    (None, 2)            0           ['z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      " tf.keras.backend.binary_crosse  (None, 5)           0           ['tf.cast_14[0][0]',             \n",
      " ntropy_8 (TFOpLambda)                                            'tf.convert_to_tensor_24[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.subtract_21 (TFOpLambd  (None, 2)           0           ['tf.math.subtract_20[0][0]',    \n",
      " a)                                                               'tf.math.exp_10[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_18 (TFOpLa  (None,)             0           ['tf.keras.backend.binary_crossen\n",
      " mbda)                                                           tropy_8[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_12 (TFOpLam  (None,)             0           ['tf.math.subtract_21[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_20 (TFOpLambd  (None,)             0           ['tf.math.reduce_mean_18[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_21 (TFOpLambd  (None,)             0           ['tf.math.reduce_sum_12[0][0]']  \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None,)             0           ['tf.math.multiply_20[0][0]',    \n",
      " ambda)                                                           'tf.math.multiply_21[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_19 (TFOpLa  ()                  0           ['tf.__operators__.add_21[0][0]']\n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " add_loss_8 (AddLoss)           ()                   0           ['tf.math.reduce_mean_19[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 585\n",
      "Trainable params: 585\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "12500/12500 [==============================] - 19s 1ms/step - loss: 0.9933 - val_loss: 1.0237\n",
      "Epoch 2/100\n",
      "12500/12500 [==============================] - 19s 1ms/step - loss: 0.9668 - val_loss: 1.0161\n",
      "Epoch 3/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9656 - val_loss: 1.0283\n",
      "Epoch 4/100\n",
      "12500/12500 [==============================] - 19s 1ms/step - loss: 0.9651 - val_loss: 1.0367\n",
      "Epoch 5/100\n",
      "12500/12500 [==============================] - 19s 1ms/step - loss: 0.9651 - val_loss: 1.0327\n",
      "Epoch 6/100\n",
      "12500/12500 [==============================] - 19s 2ms/step - loss: 0.9648 - val_loss: 1.0454\n",
      "Epoch 7/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9647 - val_loss: 1.0463\n",
      "Epoch 8/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9648 - val_loss: 1.0211\n",
      "Epoch 9/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9650 - val_loss: 1.0405\n",
      "Epoch 10/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9648 - val_loss: 1.0163\n",
      "Epoch 11/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9648 - val_loss: 1.0099\n",
      "Epoch 12/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9648 - val_loss: 1.0339\n",
      "Epoch 13/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9648 - val_loss: 1.0398\n",
      "Epoch 14/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9648 - val_loss: 1.0355\n",
      "Epoch 15/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9647 - val_loss: 1.0264\n",
      "Epoch 16/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9649 - val_loss: 1.0261\n",
      "Epoch 17/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9645 - val_loss: 1.0045\n",
      "Epoch 18/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9646 - val_loss: 1.0004\n",
      "Epoch 19/100\n",
      "12500/12500 [==============================] - 19s 2ms/step - loss: 0.9648 - val_loss: 1.0281\n",
      "Epoch 20/100\n",
      "12500/12500 [==============================] - 19s 1ms/step - loss: 0.9645 - val_loss: 1.0128\n",
      "Epoch 21/100\n",
      "12500/12500 [==============================] - 19s 1ms/step - loss: 0.9642 - val_loss: 1.0112\n",
      "Epoch 22/100\n",
      "12500/12500 [==============================] - 19s 1ms/step - loss: 0.9644 - val_loss: 1.0159\n",
      "Epoch 23/100\n",
      "12500/12500 [==============================] - 19s 1ms/step - loss: 0.9645 - val_loss: 1.0254\n",
      "Epoch 24/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9646 - val_loss: 1.0400\n",
      "Epoch 25/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9648 - val_loss: 1.0341\n",
      "Epoch 26/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0419\n",
      "Epoch 27/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9646 - val_loss: 1.0074\n",
      "Epoch 28/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9646 - val_loss: 1.0332\n",
      "Epoch 29/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9646 - val_loss: 1.0250\n",
      "Epoch 30/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9647 - val_loss: 1.0256\n",
      "Epoch 31/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9646 - val_loss: 1.0249\n",
      "Epoch 32/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9646 - val_loss: 1.0394\n",
      "Epoch 33/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9647 - val_loss: 1.0127\n",
      "Epoch 34/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9645 - val_loss: 1.0282\n",
      "Epoch 35/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9646 - val_loss: 1.0279\n",
      "Epoch 36/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0426\n",
      "Epoch 37/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0132\n",
      "Epoch 38/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9645 - val_loss: 1.0458\n",
      "Epoch 39/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0302\n",
      "Epoch 40/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0151\n",
      "Epoch 41/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9645 - val_loss: 1.0237\n",
      "Epoch 42/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9645 - val_loss: 1.0246\n",
      "Epoch 43/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0249\n",
      "Epoch 44/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9644 - val_loss: 1.0313\n",
      "Epoch 45/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0343\n",
      "Epoch 46/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9645 - val_loss: 1.0107\n",
      "Epoch 47/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0224\n",
      "Epoch 48/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0147\n",
      "Epoch 49/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0387\n",
      "Epoch 50/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9645 - val_loss: 1.0255\n",
      "Epoch 51/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0420\n",
      "Epoch 52/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9642 - val_loss: 1.0297\n",
      "Epoch 53/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0247\n",
      "Epoch 54/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0222\n",
      "Epoch 55/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9645 - val_loss: 1.0211\n",
      "Epoch 56/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0255\n",
      "Epoch 57/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0411\n",
      "Epoch 58/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0213\n",
      "Epoch 59/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0331\n",
      "Epoch 60/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9642 - val_loss: 1.0345\n",
      "Epoch 61/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0263\n",
      "Epoch 62/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0230\n",
      "Epoch 63/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9642 - val_loss: 1.0357\n",
      "Epoch 64/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0152\n",
      "Epoch 65/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0317\n",
      "Epoch 66/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0193\n",
      "Epoch 67/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9642 - val_loss: 1.0330\n",
      "Epoch 68/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0149\n",
      "Epoch 69/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0239\n",
      "Epoch 70/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9645 - val_loss: 1.0129\n",
      "Epoch 71/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0380\n",
      "Epoch 72/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9641 - val_loss: 1.0379\n",
      "Epoch 73/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0221\n",
      "Epoch 74/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9645 - val_loss: 1.0112\n",
      "Epoch 75/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0236\n",
      "Epoch 76/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0220\n",
      "Epoch 77/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0238\n",
      "Epoch 78/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0185\n",
      "Epoch 79/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9646 - val_loss: 1.0262\n",
      "Epoch 80/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9641 - val_loss: 1.0320\n",
      "Epoch 81/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9642 - val_loss: 1.0275\n",
      "Epoch 82/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0180\n",
      "Epoch 83/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0062\n",
      "Epoch 84/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9644 - val_loss: 1.0210\n",
      "Epoch 85/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9642 - val_loss: 1.0214\n",
      "Epoch 86/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9642 - val_loss: 1.0131\n",
      "Epoch 87/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9644 - val_loss: 1.0161\n",
      "Epoch 88/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9645 - val_loss: 1.0183\n",
      "Epoch 89/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9644 - val_loss: 1.0173\n",
      "Epoch 90/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0215\n",
      "Epoch 91/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0162\n",
      "Epoch 92/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0151\n",
      "Epoch 93/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9644 - val_loss: 1.0273\n",
      "Epoch 94/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0401\n",
      "Epoch 95/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9644 - val_loss: 1.0165\n",
      "Epoch 96/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0153\n",
      "Epoch 97/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0293\n",
      "Epoch 98/100\n",
      "12500/12500 [==============================] - 18s 1ms/step - loss: 0.9643 - val_loss: 1.0188\n",
      "Epoch 99/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9640 - val_loss: 1.0261\n",
      "Epoch 100/100\n",
      "12500/12500 [==============================] - 17s 1ms/step - loss: 0.9644 - val_loss: 1.0325\n",
      "15625/15625 [==============================] - 14s 861us/step\n"
     ]
    }
   ],
   "source": [
    "# newest attempt:\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Load the original dataset\n",
    "processed_data = pd.read_csv('Processed Data/processed_data.csv')\n",
    "\n",
    "# Extract the relevant columns from the dataset\n",
    "timestamp = processed_data['Timestamp'].values\n",
    "device_id = processed_data['Device ID'].values\n",
    "status = processed_data['Status'].values\n",
    "activity = processed_data['Activity'].values\n",
    "activity_status = processed_data['Activity Status'].values\n",
    "\n",
    "# Prepare the data for input into the VAE model\n",
    "X = np.stack((timestamp, device_id, status, activity, activity_status), axis=1)\n",
    "\n",
    "# Normalize the data using minMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, X, test_size=validation_split, shuffle=False)\n",
    "X_train = tf.constant(X_train, dtype=tf.float32)\n",
    "X_val = tf.constant(X_val, dtype=tf.float32)\n",
    "\n",
    "latent_dim = 2\n",
    "encoding_dim = 32\n",
    "input_shape = (X_train.shape[1],)\n",
    "\n",
    "# Set the input shape for the VAE model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(encoding_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(encoding_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(input_shape[0], activation='sigmoid')(x)\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "reconstruction_loss *= input_shape[0]\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "\n",
    "batch_size = 32\n",
    "num_samples = X_train.shape[0]\n",
    "steps_per_epoch = num_samples // batch_size\n",
    "history = vae.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val), steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Generate a fake dataset using the VAE model\n",
    "\n",
    "n_samples = len(processed_data)\n",
    "noise = np.random.normal(size=(n_samples, 5 - latent_dim))\n",
    "noise = np.concatenate([noise, np.zeros((n_samples, latent_dim))], axis=-1)\n",
    "predicted_values = vae.predict(noise)\n",
    "\n",
    "# undo the normalization\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Save the prediction data to a new file 'predicted_Data.csv'\n",
    "predicted_data = pd.DataFrame(predicted_values, columns=['Timestamp', 'Device ID', 'Status', 'Activity', 'Activity Status'])\n",
    "predicted_data.to_csv('Predictions/Aruba_17_prediction.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e3f0318fa44a63fbd15a81336d0e6b9929111f70e7cf4cecf151c11d26f00aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
