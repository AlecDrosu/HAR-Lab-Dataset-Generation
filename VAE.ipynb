{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, RepeatVector, TimeDistributed, Flatten\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"tf.keras.metrics.binary_crossentropy_5\" (type TFOpLambda).\n\nDimensions must be equal, but are 5 and 14 for '{{node tf.keras.metrics.binary_crossentropy_5/mul}} = Mul[T=DT_FLOAT](Placeholder, tf.keras.metrics.binary_crossentropy_5/Log)' with input shapes: [32,32,5], [32,14].\n\nCall arguments received by layer \"tf.keras.metrics.binary_crossentropy_5\" (type TFOpLambda):\n  • y_true=tf.Tensor(shape=(32, 32, 5), dtype=float32)\n  • y_pred=tf.Tensor(shape=(32, 14), dtype=float32)\n  • from_logits=False\n  • label_smoothing=0.0\n  • axis=-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [65], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m outputs \u001b[39m=\u001b[39m decoder(encoder(inputs)[\u001b[39m2\u001b[39m])\n\u001b[0;32m     70\u001b[0m vae \u001b[39m=\u001b[39m Model(inputs, outputs, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvae\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m reconstruction_loss \u001b[39m=\u001b[39m binary_crossentropy(inputs, outputs)\n\u001b[0;32m     73\u001b[0m reconstruction_loss \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m input_dim \u001b[39m*\u001b[39m timesteps\n\u001b[0;32m     74\u001b[0m kl_loss \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m z_log_var \u001b[39m-\u001b[39m K\u001b[39m.\u001b[39msquare(z_mean) \u001b[39m-\u001b[39m K\u001b[39m.\u001b[39mexp(z_log_var)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\layers\\core\\tf_op_layer.py:119\u001b[0m, in \u001b[0;36mKerasOpDispatcher.handle\u001b[1;34m(self, op, args, kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39m\"\"\"Handle the specified operation with the specified arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[0;32m    116\u001b[0m     \u001b[39misinstance\u001b[39m(x, keras_tensor\u001b[39m.\u001b[39mKerasTensor)\n\u001b[0;32m    117\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten([args, kwargs])\n\u001b[0;32m    118\u001b[0m ):\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m TFOpLambda(op)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNOT_SUPPORTED\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\backend.py:5688\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[1;34m(target, output, from_logits)\u001b[0m\n\u001b[0;32m   5685\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mclip_by_value(output, epsilon_, \u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m epsilon_)\n\u001b[0;32m   5687\u001b[0m \u001b[39m# Compute cross entropy from probabilities.\u001b[39;00m\n\u001b[1;32m-> 5688\u001b[0m bce \u001b[39m=\u001b[39m target \u001b[39m*\u001b[39;49m tf\u001b[39m.\u001b[39;49mmath\u001b[39m.\u001b[39;49mlog(output \u001b[39m+\u001b[39;49m epsilon())\n\u001b[0;32m   5689\u001b[0m bce \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m target) \u001b[39m*\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m output \u001b[39m+\u001b[39m epsilon())\n\u001b[0;32m   5690\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mbce\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"tf.keras.metrics.binary_crossentropy_5\" (type TFOpLambda).\n\nDimensions must be equal, but are 5 and 14 for '{{node tf.keras.metrics.binary_crossentropy_5/mul}} = Mul[T=DT_FLOAT](Placeholder, tf.keras.metrics.binary_crossentropy_5/Log)' with input shapes: [32,32,5], [32,14].\n\nCall arguments received by layer \"tf.keras.metrics.binary_crossentropy_5\" (type TFOpLambda):\n  • y_true=tf.Tensor(shape=(32, 32, 5), dtype=float32)\n  • y_pred=tf.Tensor(shape=(32, 14), dtype=float32)\n  • from_logits=False\n  • label_smoothing=0.0\n  • axis=-1"
     ]
    }
   ],
   "source": [
    "# Load the original dataset\n",
    "processed_data = pd.read_csv('Processed Data/Aruba_17/processed_data.csv')\n",
    "# only use the first 1000 rows\n",
    "processed_data = processed_data.head(3200)\n",
    "# Extract the relevant columns from the dataset\n",
    "timestamp = processed_data['Timestamp'].values\n",
    "device_id = processed_data['Device ID'].values\n",
    "status = processed_data['Status'].values\n",
    "activity = processed_data['Activity'].values\n",
    "activity_status = processed_data['Activity Status'].values\n",
    "\n",
    "# Prepare the data for input into the VAE model\n",
    "X = np.stack((timestamp, device_id, status, activity, activity_status), axis=1)\n",
    "\n",
    "# Normalize the data using minMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "timesteps = 32 # number of previous records considered\n",
    "input_dim = X.shape[1] # number of features, there are 5 features in the dataset\n",
    "num_classes = 14 # number of patters we need to recognize \"large scale\"\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, X, test_size=validation_split, shuffle=False)\n",
    "X_train = X_train.reshape((int(X_train.shape[0]/timesteps), timesteps, input_dim))\n",
    "X_val = X_val.reshape((int(X_val.shape[0]/timesteps), timesteps, input_dim))\n",
    "# you need to reshape the y_train and y_val as well\n",
    "y_train = y_train.reshape((int(y_train.shape[0]/timesteps), timesteps, input_dim))\n",
    "y_val = y_val.reshape((int(y_val.shape[0]/timesteps), timesteps, input_dim))\n",
    "\n",
    "latent_dim = 2\n",
    "encoding_dim = 32\n",
    "\n",
    "# Set the input shape for the VAE model\n",
    "inputs = Input(batch_shape=(batch_size, timesteps, input_dim), name='encoder_input')\n",
    "x = LSTM(encoding_dim, return_sequences=True)(inputs) # Add LSTM layer with return_sequences set to True\n",
    "x = LSTM(encoding_dim, return_sequences=False)(x) # Add another LSTM layer with return_sequences set to False\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "# encoder.summary()\n",
    "\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(encoding_dim, activation='relu')(latent_inputs)\n",
    "x = RepeatVector(timesteps)(x) # Repeat the hidden state timesteps times\n",
    "x = LSTM(encoding_dim, return_sequences=True)(x) # Add a LSTM layer with return_sequences set to True\n",
    "x = LSTM(encoding_dim, return_sequences=True)(x) # Add another LSTM layer with return_sequences set to True\n",
    "x = TimeDistributed(Dense(input_dim, activation='sigmoid'))(x)\n",
    "\n",
    "# Add fully connected layer to classify sequences into 14 categories\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(14, activation='softmax')(x)\n",
    "\n",
    "outputs = x\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "# decoder.summary()\n",
    "\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "reconstruction_loss *= input_dim * timesteps\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "kl_loss = tf.keras.backend.expand_dims(kl_loss, axis=-1)\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "# vae.summary()\n",
    "\n",
    "batch_size = 32\n",
    "num_samples = X_train.shape[0]\n",
    "steps_per_epoch = num_samples // batch_size\n",
    "history = vae.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val), steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Generate a fake dataset using the VAE model\n",
    "\n",
    "n_samples = len(processed_data)\n",
    "noise = np.random.normal(size=(n_samples, 5 - latent_dim))\n",
    "noise = np.concatenate([noise, np.zeros((n_samples, latent_dim))], axis=-1)\n",
    "# reshape noise to have the correct shape\n",
    "noise = noise.reshape((int(noise.shape[0]/timesteps), timesteps, input_dim))\n",
    "predicted_values = vae.predict(noise, batch_size=batch_size)\n",
    "\n",
    "predicted_values = predicted_values.reshape((predicted_values.shape[0] * predicted_values.shape[1], predicted_values.shape[2]))\n",
    "\n",
    "# undo the normalization\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# Round each of the values in the array to the nearest integer\n",
    "predicted_values = np.rint(predicted_values)\n",
    "\n",
    "# Save the prediction data to a new file 'predicted_Data.csv'\n",
    "predicted_data = pd.DataFrame(predicted_values, columns=['Timestamp', 'Device ID', 'Status', 'Activity', 'Activity Status'])\n",
    "predicted_data.to_csv('Predictions/Aruba_17_prediction.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e3f0318fa44a63fbd15a81336d0e6b9929111f70e7cf4cecf151c11d26f00aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
